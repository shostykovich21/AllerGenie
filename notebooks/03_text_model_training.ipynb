{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "580656f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cac0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny shim: Path to str, str to str  (no-op)\n",
    "def as_str(p): \n",
    "    from pathlib import Path\n",
    "    return str(p) if isinstance(p, Path) else p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cdd5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR    = Path(\"/mnt/ssd1/saumia/data/text\")\n",
    "WEIGHTS_DIR = Path(\"weights\"); WEIGHTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME  = \"microsoft/biogpt\"  # public and accessible\n",
    "NUM_LABELS  = 7                   # adjust based on your label_map.json\n",
    "MAX_LEN     = 256\n",
    "BATCH_SIZE  = 8\n",
    "EPOCHS      = 3\n",
    "LR          = 2e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9810801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " label\n",
      "Psoriasis                          50\n",
      "Varicose Veins                     50\n",
      "Typhoid                            50\n",
      "Impetigo                           50\n",
      "Fungal infection                   50\n",
      "Dengue                             50\n",
      "peptic ulcer disease               50\n",
      "Hypertension                       50\n",
      "drug reaction                      50\n",
      "allergy                            50\n",
      "urinary tract infection            50\n",
      "diabetes                           50\n",
      "Common Cold                        49\n",
      "Chicken pox                        49\n",
      "Cervical spondylosis               49\n",
      "Bronchial Asthma                   49\n",
      "gastroesophageal reflux disease    48\n",
      "Pneumonia                          47\n",
      "Migraine                           47\n",
      "Arthritis                          46\n",
      "Acne                               46\n",
      "Malaria                            44\n",
      "Dimorphic Hemorrhoids              41\n",
      "Jaundice                           38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def clean(txt):\n",
    "    txt = html.unescape(str(txt))\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "# 1) symptom to disease CSV  (already labelled)\n",
    "sym = pd.read_csv(DATA_DIR / \"symptom2disease.csv\")\n",
    "sym = sym.rename(columns={\"description\": \"text\"})\n",
    "sym[\"text\"] = sym[\"text\"].map(clean)\n",
    "sym[\"source\"] = \"sym2dis\"\n",
    "\n",
    "# 2) mtsamples  (long notes to map specialty to category)\n",
    "mts = pd.read_csv(DATA_DIR / \"mtsamples.csv\", quoting=1)      # QUOTE_ALL\n",
    "mts = mts.rename(columns={\"medical_specialty\": \"label\",\n",
    "                          \"transcription\": \"text\"})\n",
    "SPEC_TO_CAT = {\n",
    "    \"Dermatology\": \"skin\",\n",
    "    \"Bariatrics\":  \"digestive\",\n",
    "    \"Cardiovascular / Pulmonary\": \"respiratory\",\n",
    "}\n",
    "mts[\"label\"] = mts[\"label\"].map(SPEC_TO_CAT).fillna(\"other\")\n",
    "mts[\"text\"]  = mts[\"text\"].map(clean)\n",
    "mts = mts[mts[\"label\"] != \"other\"]\n",
    "mts[\"source\"] = \"mtsamples\"\n",
    "\n",
    "# --- concatenate & deduplicate ---\n",
    "df = pd.concat([sym[[\"label\",\"text\",\"source\"]],\n",
    "                mts[[\"label\",\"text\",\"source\"]]], ignore_index=True)\n",
    "df = df.drop_duplicates(subset=\"text\").reset_index(drop=True)\n",
    "print(\"Label distribution:\\n\", df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7689950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 183 food keywords\n"
     ]
    }
   ],
   "source": [
    "food_df = pd.read_csv(DATA_DIR / \"FoodData.csv\")\n",
    "FOOD2ALLERGY = dict(zip(food_df[\"Food\"].str.lower(), food_df[\"Allergy\"]))\n",
    "with open(\"food2allergy.json\", \"w\") as f:\n",
    "    json.dump(FOOD2ALLERGY, f, indent=2)\n",
    "print(\"Loaded\", len(FOOD2ALLERGY), \"food keywords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79563a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1037 | Val: 116\n"
     ]
    }
   ],
   "source": [
    "label_map_text = {lbl: i for i, lbl in enumerate(sorted(df[\"label\"].unique()))}\n",
    "with open(\"label_map_text.json\", \"w\") as f:\n",
    "    json.dump(label_map_text, f, indent=2)\n",
    "\n",
    "# Safe label map\n",
    "df = df[df[\"label\"].isin(label_map_text)].copy()\n",
    "df[\"label_id\"] = df[\"label\"].map(label_map_text).astype(int)\n",
    "\n",
    "assert df[\"label_id\"].isnull().sum() == 0, \"Some labels are unmapped!\"\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.1, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(train_df), \"| Val:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65a5067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(as_str(MODEL_NAME))\n",
    "except Exception:\n",
    "    MODEL_NAME = \"distilbert-base-uncased\"\n",
    "    tokenizer  = AutoTokenizer.from_pretrained(as_str(MODEL_NAME))\n",
    "    print(\"BioGPT fallback to DistilBERT\")\n",
    "\n",
    "def encode(frame):\n",
    "    enc = tokenizer(\n",
    "        frame[\"text\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    enc[\"labels\"] = tf.convert_to_tensor(frame[\"label_id\"].values, tf.int32)\n",
    "    return enc\n",
    "\n",
    "def make_ds(frame):\n",
    "    enc = encode(frame)\n",
    "    ds = tf.data.Dataset.from_tensor_slices({\n",
    "    \"input_ids\": enc[\"input_ids\"],\n",
    "    \"attention_mask\": enc[\"attention_mask\"],\n",
    "    \"labels\": enc[\"labels\"]\n",
    "})\n",
    "\n",
    "    return ds.shuffle(len(frame), seed=42).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "assert \"label_id\" in train_df.columns\n",
    "assert train_df[\"label_id\"].isnull().sum() == 0\n",
    "\n",
    "train_ds_text = make_ds(train_df)\n",
    "val_ds_text   = make_ds(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835e6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "130/130 [==============================] - ETA: 0s - loss: 2.8668 - accuracy: 0.3365"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saumia/miniconda3/envs/work1/lib/python3.10/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 208s 2s/step - loss: 2.8668 - accuracy: 0.3365 - val_loss: 2.2206 - val_accuracy: 0.7155\n",
      "Epoch 2/3\n",
      "130/130 [==============================] - 186s 1s/step - loss: 1.7037 - accuracy: 0.8255 - val_loss: 1.1642 - val_accuracy: 0.8190\n",
      "Epoch 3/3\n",
      "130/130 [==============================] - 185s 1s/step - loss: 0.8506 - accuracy: 0.9421 - val_loss: 0.6051 - val_accuracy: 0.8966\n",
      "✅ text model saved at weights/text_best_tf.keras\n"
     ]
    }
   ],
   "source": [
    "# %% [6] Build, compile, train\n",
    "ckpt_path = WEIGHTS_DIR / \"text_best_tf.keras\"\n",
    "\n",
    "text_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    as_str(MODEL_NAME), num_labels=len(label_map_text)\n",
    ")\n",
    "\n",
    "#   DON’T pass an explicit loss = …\n",
    "#   Just compile with an optimizer (+ optional metrics)\n",
    "text_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR, epsilon=1e-8),\n",
    "    metrics=[\"accuracy\"]          # accuracy works because model returns logits\n",
    ")\n",
    "\n",
    "history = text_model.fit(\n",
    "    train_ds_text,\n",
    "    validation_data=val_ds_text,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            str(ckpt_path),\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\"\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(as_str(WEIGHTS_DIR / \"text_tokenizer\"))\n",
    "print(\" text model saved at\", ckpt_path)\n",
    "\n",
    "# Save the fully trained text model as a .keras package\n",
    "from pathlib import Path\n",
    "\n",
    "# ensure the directory exists\n",
    "Path(\"models\").mkdir(exist_ok=True)\n",
    "\n",
    "text_model.save(\"models/text_model.keras\")\n",
    "print(\" Text model saved to models/text_model.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b327e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels NaN: 0\n",
      "Val Labels NaN: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Labels NaN:\", train_df[\"label_id\"].isnull().sum())\n",
    "print(\"Val Labels NaN:\", val_df[\"label_id\"].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42344c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "X input_ids shape: (8, 256)\n",
      "Y (labels) dtype: <dtype: 'int32'>\n",
      "Y (labels) sample: [ 8 22 11 22 19  4 23 13]\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds_text:\n",
    "    print(\"X keys:\", x.keys())\n",
    "    print(\"X input_ids shape:\", x[\"input_ids\"].shape)\n",
    "    print(\"Y (labels) dtype:\", y.dtype)\n",
    "    print(\"Y (labels) sample:\", y.numpy())\n",
    "    break  # only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e743fc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ensure the directory exists\u001b[39;00m\n\u001b[1;32m      5\u001b[0m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtext_model\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/text_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Text model saved to models/text_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9e3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
