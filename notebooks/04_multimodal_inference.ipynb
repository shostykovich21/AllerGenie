{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # core\n",
    "# import json, os, numpy as np\n",
    "# import tensorflow as tf\n",
    "# from pathlib import Path\n",
    "# import os, sys\n",
    "# # adjust the path so that the folder containing `src/` is on sys.path\n",
    "# # if your notebook lives in `project/notebooks/`, this gives you `project/`\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.insert(0, project_root)\n",
    "\n",
    "# # models\n",
    "# from src.models.image_model     import build_image_model\n",
    "# from src.models.text_model      import build_text_model\n",
    "# from src.models.multimodal_model import build_multimodal_model\n",
    "\n",
    "# # retriever\n",
    "# from src.retriever.meds_rag      import retrieve_meds\n",
    "\n",
    "# # utils\n",
    "# from src.utils.geocode_utils     import find_allergists\n",
    "# from src.utils.question_logic    import get_followup\n",
    "# from src.utils.data_utils        import seed_everything\n",
    "\n",
    "# # preprocessing\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# # ‚îÄ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# MODEL_DIR    = Path(\"models\")\n",
    "# IMG_MODEL_FP = MODEL_DIR / \"image_model.keras\"\n",
    "# TXT_MODEL_FP = MODEL_DIR / \"text_model.keras\"\n",
    "# MM_MODEL_FP  = MODEL_DIR / \"multimodal_model.keras\"\n",
    "\n",
    "# LABEL_MAP_IMG = json.loads(Path(\"label_map_image.json\").read_text())\n",
    "# LABEL_MAP_TXT = json.loads(Path(\"label_map_text.json\").read_text())\n",
    "# FOOD2ALLERGY  = json.loads(Path(\"food2allergy.json\").read_text())\n",
    "\n",
    "# FEATURE_COLS     = json.loads(Path(\"feature_cols.json\").read_text())            # metadata names\n",
    "# FEATURE_COLS_IMG = json.loads(Path(\"feature_cols_image.json\").read_text())      # if needed\n",
    "\n",
    "# # set random seeds\n",
    "# seed_everything(42)\n",
    "\n",
    "# # tokenizer\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# 0) Ensure `src/` is on Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# 1) Imports\n",
    "import json, numpy as np, tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "from src.models.multimodal_model import build_multimodal_model  # only if you prefer build+load_weights\n",
    "from src.retriever.meds_rag       import retrieve_meds\n",
    "from src.utils.question_logic     import get_followup\n",
    "from src.utils.geocode_utils      import find_allergists\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7afec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ‚îÄ‚îÄ‚îÄ Cell 2 ‚Äî Load your trained models from the `models/` folder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# from pathlib import Path\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Make sure this points to the directory where you ran model.save(...)\n",
    "# MODEL_DIR        = Path(\"models\")       # ‚Üê your .keras files are here\n",
    "# IMG_MODEL_PATH   = MODEL_DIR / \"image_model.keras\"\n",
    "# TXT_MODEL_PATH   = MODEL_DIR / \"text_model.keras\"\n",
    "# MM_MODEL_PATH    = MODEL_DIR / \"multimodal_model.keras\"\n",
    "\n",
    "# print(\"üîÑ Loading models‚Ä¶\")\n",
    "# img_model = load_model(str(IMG_MODEL_PATH), compile=False)\n",
    "# txt_model = load_model(str(TXT_MODEL_PATH), compile=False)\n",
    "# mm_model  = load_model(str(MM_MODEL_PATH),  compile=False)\n",
    "# print(\"‚úÖ Models loaded successfully.\")\n",
    "\n",
    "# 3) Build-or-Load the multimodal model\n",
    "MM_KERAS = Path(\"models/multimodal_model.keras\")\n",
    "MM_H5    = Path(\"models/multimodal_weights.h5\")\n",
    "\n",
    "# 3) Load label maps & feature columns\n",
    "\n",
    "LABEL_MAP_IMG = json.loads(Path(\"label_map_image.json\").read_text())\n",
    "ID2LABEL_IMG  = {v: k for k, v in LABEL_MAP_IMG.items()}\n",
    "FEATURE_COLS  = json.loads(Path(\"feature_cols_image.json\").read_text())\n",
    "NUM_IMG_CLASSES = len(LABEL_MAP_IMG)\n",
    "NUM_TEXT_CLASSES= len(json.loads(Path(\"label_map_text.json\").read_text()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "530e4f6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Neither models/multimodal_model.keras nor models/multimodal_weights.h5 were found. Please run your training notebook to save the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Built fusion model and loaded weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMM_H5\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeither \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMM_KERAS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMM_H5\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run your training notebook to save the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Neither models/multimodal_model.keras nor models/multimodal_weights.h5 were found. Please run your training notebook to save the model."
     ]
    }
   ],
   "source": [
    "# 4) Build‚Äêor‚ÄêLoad the multimodal model\n",
    "if MM_KERAS.exists():\n",
    "    # 4a) Preferred: load full .keras package\n",
    "    mm_model = load_model(str(MM_KERAS), compile=False)\n",
    "    print(f\"‚úÖ Loaded fusion model from {MM_KERAS}\")\n",
    "elif MM_H5.exists():\n",
    "    # 4b) Fallback: rebuild arch + load H5 weights\n",
    "    mm_model = build_multimodal_model(\n",
    "        image_input_shape=(224,224,3),\n",
    "        num_image_classes=NUM_IMG_CLASSES,\n",
    "        text_pretrained=\"distilbert-base-uncased\",\n",
    "        num_text_labels=NUM_TEXT_CLASSES,\n",
    "        max_seq_len=256,\n",
    "        metadata_dim=len(FEATURE_COLS),\n",
    "        num_classes=NUM_IMG_CLASSES,\n",
    "        dropout=0.3\n",
    "    )\n",
    "    mm_model.load_weights(str(MM_H5))\n",
    "    print(f\"‚úÖ Built fusion model and loaded weights from {MM_H5}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Neither {MM_KERAS} nor {MM_H5} were found. \"\n",
    "        \"Please run your training notebook to save the model.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46155986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded text tokenizer\n"
     ]
    }
   ],
   "source": [
    "# 5) Load text tokenizer\n",
    "TOKENIZER_DIR = Path(\"weights/text_tokenizer\")\n",
    "tokenizer     = DistilBertTokenizerFast.from_pretrained(str(TOKENIZER_DIR))\n",
    "print(\"‚úÖ Loaded text tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dc97e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Preprocessing helpers\n",
    "def preprocess_image(path: str):\n",
    "    img = load_img(path, target_size=(224,224))\n",
    "    arr = img_to_array(img)\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(arr)[None]\n",
    "\n",
    "def preprocess_text(txt: str):\n",
    "    toks = tokenizer(\n",
    "        txt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    return toks[\"input_ids\"], toks[\"attention_mask\"]\n",
    "\n",
    "def preprocess_meta(meta: dict):\n",
    "    vec = np.array([meta.get(f, 0.0) for f in FEATURE_COLS], dtype=np.float32)\n",
    "    return vec[None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "431763c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter metadata values (leave blank for 0):\n"
     ]
    }
   ],
   "source": [
    "# 7) Gather user inputs\n",
    "img_path     = input(\"üìÅ Path to image (or blank): \").strip() or None\n",
    "symptom_text = input(\"‚úçÔ∏è  Describe your symptoms (or blank): \").strip() or \"\"\n",
    "print(\"Enter metadata values (leave blank for 0):\")\n",
    "meta_input = {f: float(input(f\"  {f} = \").strip() or 0.0) for f in FEATURE_COLS}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c87a19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Preprocess inputs\n",
    "x_img = preprocess_image(img_path) if img_path else np.zeros((1,224,224,3), dtype=np.float32)\n",
    "if symptom_text:\n",
    "    x_ids, x_mask = preprocess_text(symptom_text)\n",
    "else:\n",
    "    x_ids  = tf.zeros((1,256), dtype=tf.int32)\n",
    "    x_mask = tf.zeros((1,256), dtype=tf.int32)\n",
    "x_meta = preprocess_meta(meta_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b07cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "‚ñ∂ preds shape: (6,)\n",
      "‚úÖ Valid class indices: 0 through 5\n",
      "\n",
      "üè∑Ô∏è  Diagnosis: SCC (index 4, 86.9%)\n"
     ]
    }
   ],
   "source": [
    "# 9) Run fusion prediction\n",
    "preds = mm_model.predict([x_img, x_ids, x_mask, x_meta])[0]  \n",
    "# preds is a 1-D array of length n_classes\n",
    "\n",
    "# show its shape and valid indices\n",
    "print(\"‚ñ∂ preds shape:\", preds.shape)\n",
    "min_idx, max_idx = 0, preds.shape[0] - 1\n",
    "print(f\"‚úÖ Valid class indices: {min_idx} through {max_idx}\")\n",
    "\n",
    "# pick the top class (no axis or axis=0)\n",
    "cls  = int(np.argmax(preds))   \n",
    "conf = float(preds[cls])\n",
    "diagnosis = ID2LABEL_IMG[cls]\n",
    "print(f\"\\nüè∑Ô∏è  Diagnosis: {diagnosis} (index {cls}, {conf*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a320fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Follow-up: Have you recently used any new skincare products, detergents, or soaps?\n"
     ]
    }
   ],
   "source": [
    "# 10) Follow-up question\n",
    "followup = get_followup(symptom_text, diagnosis)\n",
    "print(\"‚ùì Follow-up:\", followup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cd9f0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíä Medications / Instructions:\n",
      "  1. Directions: Acne Clearing Cleanser Acne Clearing Tonic Acne Clearing Treatment 101‚Ä¶\n",
      "  2. 1 INDICATIONS AND USAGE Varenicline tablets are indicated for use as an aid to smoking cessation treatment. Varenicline tablets are a nicotinic receptor partial agonist indicated for use as an aid to ‚Ä¶\n",
      "  3. INDICATIONS Condition listed above or as directed by the physician‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "# 11) Medications / Instructions\n",
    "meds = retrieve_meds(diagnosis, k=3)\n",
    "print(\"\\nüíä Medications / Instructions:\")\n",
    "for i, doc in enumerate(meds, 1):\n",
    "    snippet = doc.replace(\"\\n\",\" \")[:200]\n",
    "    print(f\"  {i}. {snippet}‚Ä¶\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39c1b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü©∫ Enter your coordinates to find allergists:\n",
      "Nearby allergists:\n"
     ]
    }
   ],
   "source": [
    "# 12) Nearby allergists\n",
    "print(\"\\nü©∫ Enter your coordinates to find allergists:\")\n",
    "lat = float(input(\"  latitude = \").strip())\n",
    "lng = float(input(\"  longitude = \").strip())\n",
    "places = find_allergists(lat, lng, radius_m=5000)\n",
    "print(\"Nearby allergists:\")\n",
    "for p in places:\n",
    "    name = p.get(\"name\",\"Unknown\")\n",
    "    addr = p.get(\"tags\",{}).get(\"addr:street\",\"\")\n",
    "    print(f\"  ‚Ä¢ {name} ({addr})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05265b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
