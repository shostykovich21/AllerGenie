{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba99a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# 1) Imports\n",
    "import json, numpy as np, tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "from src.models.multimodal_model import build_multimodal_model  # only if you prefer build+load_weights\n",
    "from src.retriever.meds_rag       import retrieve_meds\n",
    "from src.utils.question_logic     import get_followup\n",
    "from src.utils.geocode_utils      import find_allergists\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7afec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MM_KERAS = Path(\"models/multimodal_model.keras\")\n",
    "MM_H5    = Path(\"models/multimodal_weights.h5\")\n",
    "\n",
    "# 3) Load label maps & feature columns\n",
    "\n",
    "LABEL_MAP_IMG = json.loads(Path(\"label_map_image.json\").read_text())\n",
    "ID2LABEL_IMG  = {v: k for k, v in LABEL_MAP_IMG.items()}\n",
    "LABEL_MAP_TXT = json.loads(Path(\"label_map_text.json\").read_text())\n",
    "ID2LABEL_TXT  = {v:k for k,v in LABEL_MAP_TXT.items()}\n",
    "FEATURE_COLS  = json.loads(Path(\"feature_cols_image.json\").read_text())\n",
    "NUM_IMG_CLASSES = len(LABEL_MAP_IMG)\n",
    "NUM_TEXT_CLASSES= len(json.loads(Path(\"label_map_text.json\").read_text()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d6f11fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved H5 weights to models/multimodal.weights.h5\n"
     ]
    }
   ],
   "source": [
    "# Optional: Rebuild model and save .h5 weights for future use\n",
    "mm_model = build_multimodal_model(\n",
    "    image_input_shape=(224, 224, 3),\n",
    "    num_image_classes=NUM_IMG_CLASSES,\n",
    "    text_pretrained=\"distilbert-base-uncased\",\n",
    "    num_text_labels=NUM_TEXT_CLASSES,\n",
    "    max_seq_len=256,\n",
    "    metadata_dim=len(FEATURE_COLS),\n",
    "    num_classes=NUM_IMG_CLASSES,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Optionally load from a .keras file if needed here (but you're skipping that for now)\n",
    "\n",
    "# Save weights as .h5 for safe FastAPI loading\n",
    "mm_model.save_weights(\"models/multimodal.weights.h5\")\n",
    "print(\"‚úÖ Saved H5 weights to models/multimodal.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "530e4f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Rebuilding architecture and loading weights from .h5‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Built fusion model and loaded weights from models/multimodal.weights.h5\n"
     ]
    }
   ],
   "source": [
    "# 4) Build-or-Load the multimodal model\n",
    "from src.models.multimodal_model import build_multimodal_model\n",
    "\n",
    "MM_KERAS = Path(\"models/multimodal_model.keras\")\n",
    "MM_H5 = Path(\"models/multimodal.weights.h5\")\n",
    "\n",
    "\n",
    "if MM_H5.exists():\n",
    "    print(\"üîÑ Rebuilding architecture and loading weights from .h5‚Ä¶\")\n",
    "    mm_model = build_multimodal_model(\n",
    "        image_input_shape=(224, 224, 3),\n",
    "        num_image_classes=NUM_IMG_CLASSES,\n",
    "        text_pretrained=\"distilbert-base-uncased\",\n",
    "        num_text_labels=NUM_TEXT_CLASSES,\n",
    "        max_seq_len=256,\n",
    "        metadata_dim=len(FEATURE_COLS),\n",
    "        num_classes=NUM_IMG_CLASSES,\n",
    "        dropout=0.3\n",
    "    )\n",
    "    mm_model.load_weights(str(MM_H5))\n",
    "    print(f\"‚úÖ Built fusion model and loaded weights from {MM_H5}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è .h5 weights not found.\")\n",
    "    if MM_KERAS.exists():\n",
    "        print(\"‚ö†Ô∏è .keras file found but skipped due to Lambda deserialization issues.\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Could not find a usable model file.\\n\"\n",
    "        \"Ensure you saved weights with:\\n\"\n",
    "        \"  mm_model.save_weights('models/multimodal_weights.h5')\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c957d042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full multimodal model saved to models/multimodal_model.keras\n"
     ]
    }
   ],
   "source": [
    "mm_model.save(\"models/multimodal_model.keras\")\n",
    "print(\"‚úÖ Full multimodal model saved to models/multimodal_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "46155986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded text tokenizer\n"
     ]
    }
   ],
   "source": [
    "# 5) Load text tokenizer\n",
    "TOKENIZER_DIR = Path(\"weights/text_tokenizer\")\n",
    "tokenizer     = DistilBertTokenizerFast.from_pretrained(str(TOKENIZER_DIR))\n",
    "print(\"‚úÖ Loaded text tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0dc97e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Preprocessing helpers\n",
    "def preprocess_image(path: str):\n",
    "    img = load_img(path, target_size=(224,224))\n",
    "    arr = img_to_array(img)\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(arr)[None]\n",
    "\n",
    "def preprocess_text(txt: str):\n",
    "    toks = tokenizer(\n",
    "        txt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    return toks[\"input_ids\"], toks[\"attention_mask\"]\n",
    "\n",
    "def preprocess_meta(meta: dict):\n",
    "    vec = np.array([meta.get(f, 0.0) for f in FEATURE_COLS], dtype=np.float32)\n",
    "    return vec[None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "431763c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Please provide a valid image file path.\n",
      "‚ö†Ô∏è  Please provide a valid image file path.\n",
      "‚ö†Ô∏è  Please provide a valid image file path.\n",
      "\n",
      "Enter metadata values (leave blank for 0):\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ Cell 7 ‚Äî Gather User Inputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# 7a) Require an image path\n",
    "img_path = None\n",
    "while not img_path:\n",
    "    p = input(\"üìÅ Path to image (required): \").strip()\n",
    "    if p:\n",
    "        img_path = p\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Please provide a valid image file path.\")\n",
    "\n",
    "# 7b) Free-text symptoms (optional)\n",
    "symptom_text = input(\"‚úçÔ∏è  Describe your symptoms (or leave blank): \").strip() or \"\"\n",
    "\n",
    "# 7c) Structured metadata\n",
    "print(\"\\nEnter metadata values (leave blank for 0):\")\n",
    "meta_input = {}\n",
    "for f in FEATURE_COLS:\n",
    "    val = input(f\"  {f} = \").strip()\n",
    "    meta_input[f] = float(val) if val else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c87a19e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inputs ready:\n",
      "   ‚Ä¢ x_img: (1, 224, 224, 3)\n",
      "   ‚Ä¢ x_ids: (1, 256) x_mask: (1, 256)\n",
      "   ‚Ä¢ x_meta: (1, 18)\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ Cell 8 ‚Äî Preprocess Inputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 8a) Image ‚Üí 4D tensor\n",
    "x_img = preprocess_image(img_path)   # your helper loads + preprocesses + adds batch dim\n",
    "\n",
    "# 8b) Text ‚Üí token IDs & attention mask\n",
    "if symptom_text:\n",
    "    x_ids, x_mask = preprocess_text(symptom_text)\n",
    "else:\n",
    "    x_ids  = tf.zeros((1, 256), dtype=tf.int32)\n",
    "    x_mask = tf.zeros((1, 256), dtype=tf.int32)\n",
    "\n",
    "# 8c) Metadata ‚Üí 2D array\n",
    "x_meta = preprocess_meta(meta_input)\n",
    "\n",
    "print(\"‚úÖ Inputs ready:\")\n",
    "print(\"   ‚Ä¢ x_img:\", x_img.shape)\n",
    "print(\"   ‚Ä¢ x_ids:\", x_ids.shape, \"x_mask:\", x_mask.shape)\n",
    "print(\"   ‚Ä¢ x_meta:\", x_meta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b07cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "‚ñ∂ preds shape: (6,)\n",
      "‚úÖ Valid class indices: 0 through 5\n",
      "\n",
      "üè∑Ô∏è  Diagnosis: SCC (index 4, 72.1%)\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ Cell 9 ‚Äî Fusion-Only Prediction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# Always run the multimodal fusion model\n",
    "preds = mm_model.predict([x_img, x_ids, x_mask, x_meta])[0]\n",
    "\n",
    "print(\"‚ñ∂ preds shape:\", preds.shape)\n",
    "print(f\"‚úÖ Valid class indices: 0 through {preds.shape[0] - 1}\")\n",
    "\n",
    "cls       = int(np.argmax(preds))\n",
    "conf      = float(preds[cls])\n",
    "diagnosis = ID2LABEL_IMG[cls]\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è  Diagnosis: {diagnosis} (index {cls}, {conf*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a320fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç Thanks for your answer!\n"
     ]
    }
   ],
   "source": [
    "# 10) Interactive follow-up question(s)\n",
    "from src.utils.question_logic import FOLLOWUP_BANK, LABEL_TO_BUCKET\n",
    "\n",
    "# determine which bucket the diagnosis lives in\n",
    "bucket = LABEL_TO_BUCKET.get(diagnosis)  \n",
    "\n",
    "# primary question\n",
    "primary_q = FOLLOWUP_BANK.get(bucket, \"Could you tell me when the symptoms started?\")\n",
    "# if it‚Äôs a ‚Äúskin‚Äù issue, also ask the digestive/food question\n",
    "if bucket == \"skin\":\n",
    "    secondary_q = FOLLOWUP_BANK[\"digestive\"]\n",
    "    combined_q = f\"{primary_q} Also: {secondary_q}\"\n",
    "    user_followup = input(f\"‚ùì {combined_q}\\nYour answer: \").strip()\n",
    "else:\n",
    "    user_followup = input(f\"‚ùì {primary_q}\\nYour answer: \").strip()\n",
    "\n",
    "print(\"üëç Thanks for your answer!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cd9f0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíä Medications / Instructions:\n",
      "  1. Directions: Acne Clearing Cleanser Acne Clearing Tonic Acne Clearing Treatment 101‚Ä¶\n",
      "  2. 1 INDICATIONS AND USAGE Varenicline tablets are indicated for use as an aid to smoking cessation treatment. Varenicline tablets are a nicotinic receptor partial agonist indicated for use as an aid to ‚Ä¶\n",
      "  3. INDICATIONS Condition listed above or as directed by the physician‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "# 11) Medications / Instructions\n",
    "meds = retrieve_meds(diagnosis, k=3)\n",
    "print(\"\\nüíä Medications / Instructions:\")\n",
    "for i, doc in enumerate(meds, 1):\n",
    "    snippet = doc.replace(\"\\n\",\" \")[:200]\n",
    "    print(f\"  {i}. {snippet}‚Ä¶\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "39c1b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü©∫ Enter your coordinates to find allergists:\n",
      "Nearby allergists:\n"
     ]
    }
   ],
   "source": [
    "# 12) Nearby allergists\n",
    "print(\"\\nü©∫ Enter your coordinates to find allergists:\")\n",
    "lat = float(input(\"  latitude = \").strip())\n",
    "lng = float(input(\"  longitude = \").strip())\n",
    "places = find_allergists(lat, lng, radius_m=5000)\n",
    "print(\"Nearby allergists:\")\n",
    "for p in places:\n",
    "    name = p.get(\"name\",\"Unknown\")\n",
    "    addr = p.get(\"tags\",{}).get(\"addr:street\",\"\")\n",
    "    print(f\"  ‚Ä¢ {name} ({addr})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05265b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:92\u001b[0;36m\u001b[0m\n\u001b[0;31m    }\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# <!DOCTYPE html>\n",
    "# <html lang=\"en\">\n",
    "# <head>\n",
    "#   <meta charset=\"UTF-8\">\n",
    "#   <title>AllerGenie Multimodal Demo</title>\n",
    "#   <style>\n",
    "#     body { font-family: sans-serif; max-width: 800px; margin: 2em auto; }\n",
    "#     label { display: block; margin-top: 1em; }\n",
    "#     input, textarea, select { width: 100%; padding: 0.5em; margin-top: 0.2em; }\n",
    "#     button { margin-top: 1.5em; padding: 0.7em 1.5em; font-size: 1em; }\n",
    "#     #results { margin-top: 2em; padding: 1em; border: 1px solid #ccc; }\n",
    "#   </style>\n",
    "# </head>\n",
    "# <body>\n",
    "\n",
    "#   <h1>AllerGenie Multimodal Inference</h1>\n",
    "\n",
    "#   <!-- 1) Image Upload -->\n",
    "#   <label>\n",
    "#     üìÅ Upload lesion image:\n",
    "#     <input type=\"file\" id=\"imgInput\" accept=\"image/*\">\n",
    "#   </label>\n",
    "\n",
    "#   <!-- 2) Symptom Text -->\n",
    "#   <label>\n",
    "#     ‚úçÔ∏è Describe your symptoms:\n",
    "#     <textarea id=\"symptomText\" rows=\"3\" placeholder=\"e.g. 'I have had a red, itchy patch on my arm‚Ä¶'\"></textarea>\n",
    "#   </label>\n",
    "\n",
    "#   <!-- 3) Metadata Fields -->\n",
    "#   <fieldset>\n",
    "#     <legend>Enter metadata values (leave blank for 0)</legend>\n",
    "    \n",
    "#     <label>\n",
    "#       Age (years):\n",
    "#       <input type=\"number\" step=\"0.1\" id=\"age\" placeholder=\"e.g. 45\">\n",
    "#     </label>\n",
    "\n",
    "#     <label>\n",
    "#       Diameter 1 (mm):\n",
    "#       <input type=\"number\" step=\"0.1\" id=\"diameter_1\" placeholder=\"e.g. 12\">\n",
    "#     </label>\n",
    "\n",
    "#     <label>\n",
    "#       Diameter 2 (mm):\n",
    "#       <input type=\"number\" step=\"0.1\" id=\"diameter_2\" placeholder=\"e.g. 8\">\n",
    "#     </label>\n",
    "\n",
    "#     <label>\n",
    "#       Gender:\n",
    "#       <select id=\"gender_M\">\n",
    "#         <option value=\"0\">Female</option>\n",
    "#         <option value=\"1\">Male</option>\n",
    "#       </select>\n",
    "#     </label>\n",
    "\n",
    "#     <!-- Region one-hots -->\n",
    "#     <label>Region:</label>\n",
    "#     <div style=\"display: flex; flex-wrap: wrap; gap: 0.5em;\">\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_ABDOMEN\"> Abdomen</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_ARM\"> Arm</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_BACK\"> Back</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_CHEST\"> Chest</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_EAR\"> Ear</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_FACE\"> Face</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_FOOT\"> Foot</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_FOREARM\"> Forearm</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_HAND\"> Hand</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_LIP\"> Lip</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_NECK\"> Neck</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_NOSE\"> Nose</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_SCALP\"> Scalp</label>\n",
    "#       <label><input type=\"radio\" name=\"region\" value=\"region_THIGH\"> Thigh</label>\n",
    "#     </div>\n",
    "#   </fieldset>\n",
    "\n",
    "#   <button id=\"submitBtn\">ü©∫ Submit for Diagnosis</button>\n",
    "\n",
    "#   <div id=\"results\" hidden>\n",
    "#     <h2>Results</h2>\n",
    "#     <p id=\"diagnosis\"></p>\n",
    "#     <p id=\"confidence\"></p>\n",
    "#     <p id=\"followup\"></p>\n",
    "#   </div>\n",
    "\n",
    "#   <script>\n",
    "#     document.getElementById('submitBtn').addEventListener('click', async () => {\n",
    "#       const imgFile = document.getElementById('imgInput').files[0];\n",
    "#       if (!imgFile) {\n",
    "#         alert(\"Please upload an image.\");\n",
    "#         return;\n",
    "#       }\n",
    "\n",
    "#       // gather metadata\n",
    "#       const meta = {\n",
    "#         age: parseFloat(document.getElementById('age').value) || 0.0,\n",
    "#         diameter_1: parseFloat(document.getElementById('diameter_1').value) || 0.0,\n",
    "#         diameter_2: parseFloat(document.getElementById('diameter_2').value) || 0.0,\n",
    "#         gender_M: parseFloat(document.getElementById('gender_M').value),\n",
    "#       };\n",
    "#       // set one-hot region\n",
    "#       const regionEls = document.querySelectorAll('input[name=\"region\"]');\n",
    "#       regionEls.forEach(el => meta[el.value] = (el.checked ? 1.0 : 0.0));\n",
    "\n",
    "#       const symptom_text = document.getElementById('symptomText').value;\n",
    "\n",
    "#       // build FormData\n",
    "#       const form = new FormData();\n",
    "#       form.append('image', imgFile);\n",
    "#       form.append('symptom_text', symptom_text);\n",
    "#       form.append('metadata', JSON.stringify(meta));\n",
    "\n",
    "#       // call your backend API\n",
    "#       const resp = await fetch('/api/infer', {\n",
    "#         method: 'POST',\n",
    "#         body: form\n",
    "#       });\n",
    "#       if (!resp.ok) {\n",
    "#         alert(\"Error: \" + resp.statusText);\n",
    "#         return;\n",
    "#       }\n",
    "#       const { diagnosis, confidence, followup } = await resp.json();\n",
    "\n",
    "#       // display\n",
    "#       document.getElementById('diagnosis').innerText = `Diagnosis: ${diagnosis}`;\n",
    "#       document.getElementById('confidence').innerText = `Confidence: ${(confidence*100).toFixed(1)}%`;\n",
    "#       document.getElementById('followup').innerText = `Follow-up: ${followup}`;\n",
    "#       document.getElementById('results').hidden = false;\n",
    "#     });\n",
    "#   </script>\n",
    "# </body>\n",
    "# </html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
