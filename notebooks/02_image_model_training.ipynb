{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d6de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ed1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 ── Imports & Paths\n",
    "import json, pandas as pd, numpy as np, tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Where your allergy images live:\n",
    "IMG_DIRS    = [\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_1\"),\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_2\"),\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_3\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc4d2cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 classes  |  1184 train  /  297 val samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 ── Build & Clean DataFrame (final, with NaN‐drop before split)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Read your metadata CSV\n",
    "meta_df = pd.read_csv(\"/mnt/ssd1/saumia/data/text/metadata.csv\")\n",
    "\n",
    "# 2) Gather rows by matching image filenames to metadata\n",
    "rows = []\n",
    "for img_dir in IMG_DIRS:\n",
    "    for p in img_dir.glob(\"*.png\"):\n",
    "        parts = p.stem.split(\"_\")\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        try:\n",
    "            lesion_id = int(parts[2])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        m = meta_df[meta_df[\"lesion_id\"] == lesion_id]\n",
    "        if m.empty:\n",
    "            continue\n",
    "        m = m.iloc[0]\n",
    "        row = {\n",
    "            \"path\": str(p),\n",
    "            \"label\": m[\"diagnostic\"],\n",
    "            \"age\": m[\"age\"],\n",
    "            \"diameter_1\": m[\"diameter_1\"],\n",
    "            \"diameter_2\": m[\"diameter_2\"],\n",
    "            \"gender_M\": 1.0 if str(m[\"gender\"]).upper() == \"MALE\" else 0.0,\n",
    "            \"region\": m[\"region\"],\n",
    "        }\n",
    "        # six boolean cols\n",
    "        for col in [\"itch\", \"bleed\", \"elevation\", \"changed\", \"hurt\", \"grew\"]:\n",
    "            row[col] = 1.0 if bool(m.get(col)) else 0.0\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "assert not df.empty, \"No images matched metadata!\"\n",
    "\n",
    "# 3) Drop nuisance features\n",
    "constant_cols = [\"itch\", \"bleed\", \"elevation\", \"changed\", \"hurt\", \"grew\"]\n",
    "df = df.drop(columns=constant_cols, errors=\"ignore\")\n",
    "\n",
    "# 4) Normalize continuous features to [0,1]\n",
    "for col in [\"age\", \"diameter_1\", \"diameter_2\"]:\n",
    "    if col in df:\n",
    "        mn, mx = df[col].min(), df[col].max()\n",
    "        df[col] = (df[col] - mn) / (mx - mn + 1e-8)\n",
    "\n",
    "# 5) One‐hot encode the region column\n",
    "df = pd.get_dummies(df, columns=[\"region\"], dtype=\"float32\")\n",
    "\n",
    "# 6) Create label map and feature‐column list\n",
    "classes   = sorted(df[\"label\"].unique())\n",
    "label_map = {c: i for i, c in enumerate(classes)}\n",
    "feat_cols = [c for c in df.columns if c not in {\"path\", \"label\"}]\n",
    "\n",
    "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "# 7) Persist your mappings\n",
    "with open(\"label_map_image.json\", \"w\")    as f: json.dump(label_map, f, indent=2)\n",
    "with open(\"feature_cols_image.json\", \"w\") as f: json.dump(feat_cols, f, indent=2)\n",
    "\n",
    "# 8) Drop any rows with missing feature or label values\n",
    "df = df.dropna(subset=feat_cols + [\"label_id\"]).reset_index(drop=True)\n",
    "\n",
    "# 9) Stratified train/validation split\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 10) Ensure correct dtypes\n",
    "train_df[feat_cols] = train_df[feat_cols].astype(\"float32\")\n",
    "val_df[feat_cols]   = val_df[feat_cols].astype(\"float32\")\n",
    "train_df[\"label_id\"] = train_df[\"label_id\"].astype(\"int32\")\n",
    "val_df[\"label_id\"]   = val_df[\"label_id\"].astype(\"int32\")\n",
    "\n",
    "print(f\"{len(classes)} classes  |  {len(train_df)} train  /  {len(val_df)} val samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ada60213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>diameter_1</th>\n",
       "      <th>diameter_2</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>region_ABDOMEN</th>\n",
       "      <th>region_ARM</th>\n",
       "      <th>region_BACK</th>\n",
       "      <th>region_CHEST</th>\n",
       "      <th>region_EAR</th>\n",
       "      <th>region_FACE</th>\n",
       "      <th>region_FOOT</th>\n",
       "      <th>region_FOREARM</th>\n",
       "      <th>region_HAND</th>\n",
       "      <th>region_LIP</th>\n",
       "      <th>region_NECK</th>\n",
       "      <th>region_NOSE</th>\n",
       "      <th>region_SCALP</th>\n",
       "      <th>region_THIGH</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.00000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.650031</td>\n",
       "      <td>0.117426</td>\n",
       "      <td>0.124278</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.009291</td>\n",
       "      <td>0.086993</td>\n",
       "      <td>0.11402</td>\n",
       "      <td>0.128378</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>0.231419</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.130068</td>\n",
       "      <td>0.048142</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>0.059966</td>\n",
       "      <td>0.096284</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>1.461993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.166620</td>\n",
       "      <td>0.086309</td>\n",
       "      <td>0.082412</td>\n",
       "      <td>0.500166</td>\n",
       "      <td>0.095979</td>\n",
       "      <td>0.281944</td>\n",
       "      <td>0.31797</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.180749</td>\n",
       "      <td>0.421918</td>\n",
       "      <td>0.100204</td>\n",
       "      <td>0.336520</td>\n",
       "      <td>0.214156</td>\n",
       "      <td>0.119012</td>\n",
       "      <td>0.237525</td>\n",
       "      <td>0.295105</td>\n",
       "      <td>0.058050</td>\n",
       "      <td>0.180749</td>\n",
       "      <td>1.382260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age   diameter_1   diameter_2     gender_M  region_ABDOMEN  \\\n",
       "count  1184.000000  1184.000000  1184.000000  1184.000000     1184.000000   \n",
       "mean      0.650031     0.117426     0.124278     0.493243        0.009291   \n",
       "std       0.166620     0.086309     0.082412     0.500166        0.095979   \n",
       "min       0.079545     0.000000     0.000000     0.000000        0.000000   \n",
       "25%       0.545455     0.070000     0.071429     0.000000        0.000000   \n",
       "50%       0.659091     0.100000     0.100000     0.000000        0.000000   \n",
       "75%       0.772727     0.150000     0.142857     1.000000        0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000        1.000000   \n",
       "\n",
       "        region_ARM  region_BACK  region_CHEST   region_EAR  region_FACE  \\\n",
       "count  1184.000000   1184.00000   1184.000000  1184.000000  1184.000000   \n",
       "mean      0.086993      0.11402      0.128378     0.033784     0.231419   \n",
       "std       0.281944      0.31797      0.334652     0.180749     0.421918   \n",
       "min       0.000000      0.00000      0.000000     0.000000     0.000000   \n",
       "25%       0.000000      0.00000      0.000000     0.000000     0.000000   \n",
       "50%       0.000000      0.00000      0.000000     0.000000     0.000000   \n",
       "75%       0.000000      0.00000      0.000000     0.000000     0.000000   \n",
       "max       1.000000      1.00000      1.000000     1.000000     1.000000   \n",
       "\n",
       "       region_FOOT  region_FOREARM  region_HAND   region_LIP  region_NECK  \\\n",
       "count  1184.000000     1184.000000  1184.000000  1184.000000  1184.000000   \n",
       "mean      0.010135        0.130068     0.048142     0.014358     0.059966   \n",
       "std       0.100204        0.336520     0.214156     0.119012     0.237525   \n",
       "min       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000        1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       region_NOSE  region_SCALP  region_THIGH     label_id  \n",
       "count  1184.000000   1184.000000   1184.000000  1184.000000  \n",
       "mean      0.096284      0.003378      0.033784     1.461993  \n",
       "std       0.295105      0.058050      0.180749     1.382260  \n",
       "min       0.000000      0.000000      0.000000     0.000000  \n",
       "25%       0.000000      0.000000      0.000000     1.000000  \n",
       "50%       0.000000      0.000000      0.000000     1.000000  \n",
       "75%       0.000000      0.000000      0.000000     1.000000  \n",
       "max       1.000000      1.000000      1.000000     5.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "462bccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dropped rows with missing features: now 1184 train / 297 val\n",
      "Image batch shape: (16, 224, 224, 3)\n",
      "Meta batch shape: (16, 18)\n",
      "Labels: [3 2 1 3 1 1 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 05:52:43.510408: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 ── tf.data Pipelines\n",
    "IMG_SIZE   = (224,224)\n",
    "BATCH_SIZE = 16\n",
    "AUTOTUNE   = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(img)\n",
    "\n",
    "def make_dataset(df, shuffle=True):\n",
    "    paths  = df[\"path\"].values\n",
    "    metas  = df[feat_cols].values\n",
    "    labels = df[\"label_id\"].values\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, metas, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(df), seed=42)\n",
    "    def _load(path, meta, label):\n",
    "        return {\"image\": preprocess_image(path), \"meta\": meta}, label\n",
    "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(train_df, shuffle=True)\n",
    "val_ds   = make_dataset(val_df,   shuffle=False)\n",
    "# Clean and coerce feature types\n",
    "for col in feat_cols:\n",
    "    train_df[col] = pd.to_numeric(train_df[col], errors=\"coerce\")\n",
    "    val_df[col]   = pd.to_numeric(val_df[col], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing features\n",
    "train_df = train_df.dropna(subset=feat_cols + [\"label_id\"]).reset_index(drop=True)\n",
    "val_df   = val_df.dropna(subset=feat_cols + [\"label_id\"]).reset_index(drop=True)\n",
    "print(f\"✅ Dropped rows with missing features: now {len(train_df)} train / {len(val_df)} val\")\n",
    "\n",
    "# Ensure all features are float32\n",
    "train_df[feat_cols] = train_df[feat_cols].astype(\"float32\")\n",
    "val_df[feat_cols]   = val_df[feat_cols].astype(\"float32\")\n",
    "\n",
    "# Optional: Clip to [0, 1] for safety\n",
    "train_df[feat_cols] = train_df[feat_cols].clip(0.0, 1.0)\n",
    "val_df[feat_cols]   = val_df[feat_cols].clip(0.0, 1.0)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "for (batch_x, batch_y) in train_ds.take(1):\n",
    "    print(\"Image batch shape:\", batch_x[\"image\"].shape)\n",
    "    print(\"Meta batch shape:\",  batch_x[\"meta\"].shape)\n",
    "    print(\"Labels:\", batch_y.numpy()[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1423a37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"image_meta_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"image_meta_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ meta (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │ meta[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnetb0      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1344</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ efficientnetb0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">172,160</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ meta (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │         \u001b[38;5;34m72\u001b[0m │ meta[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,216\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnetb0      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │  \u001b[38;5;34m4,049,571\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1344\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ efficientnetb0[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m172,160\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │        \u001b[38;5;34m774\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,223,793</span> (16.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,223,793\u001b[0m (16.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">174,186</span> (680.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m174,186\u001b[0m (680.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,607</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,607\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4 ── Build & Compile the Model (Improved Version)\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load EfficientNetB0 with pretrained ImageNet weights\n",
    "base_img = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False, pooling=\"avg\", weights=\"imagenet\")\n",
    "base_img.trainable = False  # Freeze to prevent overfitting on small data\n",
    "\n",
    "# Inputs\n",
    "img_in  = layers.Input(shape=IMG_SIZE + (3,), name=\"image\")\n",
    "meta_in = layers.Input(shape=(len(feat_cols),), name=\"meta\")\n",
    "\n",
    "# Image path\n",
    "x1 = base_img(img_in)\n",
    "\n",
    "# Metadata path\n",
    "x2 = layers.BatchNormalization()(meta_in)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x2)\n",
    "x2 = layers.Dropout(0.3)(x2)\n",
    "\n",
    "# Combine\n",
    "x = layers.concatenate([x1, x2])\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "out = layers.Dense(len(classes), activation=\"softmax\")(x)\n",
    "\n",
    "# Build model\n",
    "model = Model([img_in, meta_in], out, name=\"image_meta_model\")\n",
    "\n",
    "\n",
    "\n",
    "# Compile with gradient clipping and learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50cd4dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Any NaNs in final train data? False\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Any NaNs in final train data?\", train_df[feat_cols].isnull().any().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b69415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.2888 - loss: 1.7759\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55892, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 302ms/step - accuracy: 0.2909 - loss: 1.7726 - val_accuracy: 0.5589 - val_loss: 1.2571 - learning_rate: 1.0000e-04\n",
      "Epoch 2/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.5467 - loss: 1.2526\n",
      "Epoch 2: val_accuracy improved from 0.55892 to 0.56566, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 286ms/step - accuracy: 0.5468 - loss: 1.2528 - val_accuracy: 0.5657 - val_loss: 1.2153 - learning_rate: 1.0000e-04\n",
      "Epoch 3/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.5739 - loss: 1.2095\n",
      "Epoch 3: val_accuracy improved from 0.56566 to 0.57239, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 276ms/step - accuracy: 0.5739 - loss: 1.2093 - val_accuracy: 0.5724 - val_loss: 1.1854 - learning_rate: 1.0000e-04\n",
      "Epoch 4/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.5860 - loss: 1.1934\n",
      "Epoch 4: val_accuracy did not improve from 0.57239\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 276ms/step - accuracy: 0.5859 - loss: 1.1929 - val_accuracy: 0.5690 - val_loss: 1.1525 - learning_rate: 1.0000e-04\n",
      "Epoch 5/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.6021 - loss: 1.1178\n",
      "Epoch 5: val_accuracy improved from 0.57239 to 0.58586, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 283ms/step - accuracy: 0.6020 - loss: 1.1177 - val_accuracy: 0.5859 - val_loss: 1.1317 - learning_rate: 1.0000e-04\n",
      "Epoch 6/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.6178 - loss: 1.0764\n",
      "Epoch 6: val_accuracy improved from 0.58586 to 0.60606, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 282ms/step - accuracy: 0.6177 - loss: 1.0765 - val_accuracy: 0.6061 - val_loss: 1.1074 - learning_rate: 1.0000e-04\n",
      "Epoch 7/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.6048 - loss: 1.0642\n",
      "Epoch 7: val_accuracy improved from 0.60606 to 0.60943, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 281ms/step - accuracy: 0.6048 - loss: 1.0643 - val_accuracy: 0.6094 - val_loss: 1.0898 - learning_rate: 1.0000e-04\n",
      "Epoch 8/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.6554 - loss: 0.9736\n",
      "Epoch 8: val_accuracy did not improve from 0.60943\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 268ms/step - accuracy: 0.6550 - loss: 0.9742 - val_accuracy: 0.6027 - val_loss: 1.0729 - learning_rate: 1.0000e-04\n",
      "Epoch 9/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.6605 - loss: 1.0139\n",
      "Epoch 9: val_accuracy did not improve from 0.60943\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 273ms/step - accuracy: 0.6604 - loss: 1.0137 - val_accuracy: 0.6094 - val_loss: 1.0646 - learning_rate: 1.0000e-04\n",
      "Epoch 10/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.6332 - loss: 0.9909\n",
      "Epoch 10: val_accuracy did not improve from 0.60943\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 288ms/step - accuracy: 0.6333 - loss: 0.9909 - val_accuracy: 0.6094 - val_loss: 1.0558 - learning_rate: 1.0000e-04\n",
      "Epoch 11/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6536 - loss: 0.9439\n",
      "Epoch 11: val_accuracy improved from 0.60943 to 0.61616, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 293ms/step - accuracy: 0.6537 - loss: 0.9441 - val_accuracy: 0.6162 - val_loss: 1.0341 - learning_rate: 1.0000e-04\n",
      "Epoch 12/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6894 - loss: 0.8994\n",
      "Epoch 12: val_accuracy did not improve from 0.61616\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 284ms/step - accuracy: 0.6891 - loss: 0.8998 - val_accuracy: 0.6162 - val_loss: 1.0330 - learning_rate: 1.0000e-04\n",
      "Epoch 13/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.6849 - loss: 0.8983\n",
      "Epoch 13: val_accuracy did not improve from 0.61616\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 282ms/step - accuracy: 0.6850 - loss: 0.8983 - val_accuracy: 0.6162 - val_loss: 1.0178 - learning_rate: 1.0000e-04\n",
      "Epoch 14/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.6766 - loss: 0.8689\n",
      "Epoch 14: val_accuracy improved from 0.61616 to 0.62626, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 289ms/step - accuracy: 0.6767 - loss: 0.8692 - val_accuracy: 0.6263 - val_loss: 1.0181 - learning_rate: 1.0000e-04\n",
      "Epoch 15/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.6651 - loss: 0.8962\n",
      "Epoch 15: val_accuracy improved from 0.62626 to 0.63636, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 291ms/step - accuracy: 0.6655 - loss: 0.8957 - val_accuracy: 0.6364 - val_loss: 1.0049 - learning_rate: 1.0000e-04\n",
      "Epoch 16/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.7124 - loss: 0.7881\n",
      "Epoch 16: val_accuracy improved from 0.63636 to 0.63973, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 288ms/step - accuracy: 0.7122 - loss: 0.7887 - val_accuracy: 0.6397 - val_loss: 0.9990 - learning_rate: 1.0000e-04\n",
      "Epoch 17/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7215 - loss: 0.7934\n",
      "Epoch 17: val_accuracy did not improve from 0.63973\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 284ms/step - accuracy: 0.7214 - loss: 0.7938 - val_accuracy: 0.6330 - val_loss: 0.9933 - learning_rate: 1.0000e-04\n",
      "Epoch 18/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.7142 - loss: 0.8174\n",
      "Epoch 18: val_accuracy did not improve from 0.63973\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 279ms/step - accuracy: 0.7142 - loss: 0.8174 - val_accuracy: 0.6330 - val_loss: 0.9941 - learning_rate: 1.0000e-04\n",
      "Epoch 19/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7326 - loss: 0.7625\n",
      "Epoch 19: val_accuracy did not improve from 0.63973\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 276ms/step - accuracy: 0.7322 - loss: 0.7632 - val_accuracy: 0.6397 - val_loss: 0.9713 - learning_rate: 1.0000e-04\n",
      "Epoch 20/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.7167 - loss: 0.7959\n",
      "Epoch 20: val_accuracy improved from 0.63973 to 0.65320, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 286ms/step - accuracy: 0.7167 - loss: 0.7960 - val_accuracy: 0.6532 - val_loss: 0.9691 - learning_rate: 1.0000e-04\n",
      "Epoch 21/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7387 - loss: 0.7392\n",
      "Epoch 21: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 278ms/step - accuracy: 0.7384 - loss: 0.7399 - val_accuracy: 0.6465 - val_loss: 0.9703 - learning_rate: 1.0000e-04\n",
      "Epoch 22/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7310 - loss: 0.7563\n",
      "Epoch 22: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 280ms/step - accuracy: 0.7309 - loss: 0.7563 - val_accuracy: 0.6532 - val_loss: 0.9683 - learning_rate: 1.0000e-04\n",
      "Epoch 23/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7542 - loss: 0.7118\n",
      "Epoch 23: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 279ms/step - accuracy: 0.7540 - loss: 0.7123 - val_accuracy: 0.6431 - val_loss: 0.9674 - learning_rate: 1.0000e-04\n",
      "Epoch 24/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.7309 - loss: 0.7327\n",
      "Epoch 24: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 273ms/step - accuracy: 0.7311 - loss: 0.7327 - val_accuracy: 0.6498 - val_loss: 0.9614 - learning_rate: 1.0000e-04\n",
      "Epoch 25/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7543 - loss: 0.7010\n",
      "Epoch 25: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 278ms/step - accuracy: 0.7543 - loss: 0.7010 - val_accuracy: 0.6431 - val_loss: 0.9659 - learning_rate: 1.0000e-04\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ── Train (Improved Version)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"image_meta_best.keras\",   # Save only the best\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=33,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3180388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 1) Settings\n",
    "DATA_DIR    = \"/mnt/ssd1/saumia/data/images/IMG_CLASSES\"\n",
    "BATCH_SIZE  = 32\n",
    "IMG_SIZE    = (224, 224)\n",
    "EPOCHS      = 35\n",
    "PATIENCE    = 5\n",
    "WEIGHTS_FP  = \"models/img_classes_weights.h5\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "293f8c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 classes: ['1. Eczema 1677', '10. Warts Molluscum and other Viral Infections - 2103', '2. Melanoma 15.75k', '3. Atopic Dermatitis - 1.25k', '4. Basal Cell Carcinoma (BCC) 3323', '5. Melanocytic Nevi (NV) - 7970', '6. Benign Keratosis-like Lesions (BKL) 2624', '7. Psoriasis pictures Lichen Planus and related diseases - 2k', '8. Seborrheic Keratoses and other Benign Tumors - 1.8k', '9. Tinea Ringworm Candidiasis and other Fungal Infections - 1.7k']\n"
     ]
    }
   ],
   "source": [
    "# 2) Discover classes\n",
    "class_names = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
    "num_classes = len(class_names)\n",
    "print(f\"Found {num_classes} classes: {class_names}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab3b7144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21726 images belonging to 10 classes.\n",
      "Found 5427 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# 3) Generators with augmentation & split\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02ef93a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.6189269746646795, 1: 1.290909090909091, 2: 0.8648885350318471, 3: 2.1596421471172964, 4: 0.8170740880030086, 5: 0.3407465495608532, 6: 1.3056490384615385, 7: 1.3215328467153284, 8: 1.469959404600812, 9: 1.5951541850220263}\n"
     ]
    }
   ],
   "source": [
    "# 4) Compute class weights to handle imbalance\n",
    "labels = train_gen.classes\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = {i: w for i, w in enumerate(cw)}\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78f36a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saumia/miniconda3/envs/work1/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"img_classes_cnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"img_classes_cnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186624</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186624</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,888,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ img_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186624\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186624\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m23,888,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ img_output (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,908,682</span> (91.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,908,682\u001b[0m (91.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,908,682</span> (91.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,908,682\u001b[0m (91.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5) Define the CNN model\n",
    "model_2 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=IMG_SIZE + (3,)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax', name=\"img_output\")\n",
    "], name=\"img_classes_cnn\")\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c9e31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Callbacks: checkpoint & early stopping\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        WEIGHTS_FP,\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b1b8e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saumia/miniconda3/envs/work1/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751ms/step - accuracy: 0.3409 - loss: 2.4410\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31196, saving model to models/img_classes_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 833ms/step - accuracy: 0.3410 - loss: 2.4402 - val_accuracy: 0.3120 - val_loss: 1.9778\n",
      "Epoch 2/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748ms/step - accuracy: 0.4933 - loss: 1.5493\n",
      "Epoch 2: val_accuracy did not improve from 0.31196\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 825ms/step - accuracy: 0.4933 - loss: 1.5492 - val_accuracy: 0.2828 - val_loss: 1.9565\n",
      "Epoch 3/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759ms/step - accuracy: 0.5236 - loss: 1.4489\n",
      "Epoch 3: val_accuracy did not improve from 0.31196\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 839ms/step - accuracy: 0.5236 - loss: 1.4489 - val_accuracy: 0.3007 - val_loss: 1.9067\n",
      "Epoch 4/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758ms/step - accuracy: 0.5463 - loss: 1.3907\n",
      "Epoch 4: val_accuracy did not improve from 0.31196\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 836ms/step - accuracy: 0.5464 - loss: 1.3907 - val_accuracy: 0.2834 - val_loss: 2.1118\n",
      "Epoch 5/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754ms/step - accuracy: 0.5640 - loss: 1.3378\n",
      "Epoch 5: val_accuracy improved from 0.31196 to 0.33241, saving model to models/img_classes_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 836ms/step - accuracy: 0.5640 - loss: 1.3378 - val_accuracy: 0.3324 - val_loss: 1.9886\n",
      "Epoch 6/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755ms/step - accuracy: 0.5764 - loss: 1.2895\n",
      "Epoch 6: val_accuracy did not improve from 0.33241\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 833ms/step - accuracy: 0.5764 - loss: 1.2895 - val_accuracy: 0.3118 - val_loss: 2.0001\n",
      "Epoch 7/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761ms/step - accuracy: 0.5821 - loss: 1.2873\n",
      "Epoch 7: val_accuracy did not improve from 0.33241\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 836ms/step - accuracy: 0.5821 - loss: 1.2873 - val_accuracy: 0.3219 - val_loss: 1.9633\n",
      "Epoch 8/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755ms/step - accuracy: 0.6046 - loss: 1.2329\n",
      "Epoch 8: val_accuracy did not improve from 0.33241\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 832ms/step - accuracy: 0.6046 - loss: 1.2329 - val_accuracy: 0.3086 - val_loss: 2.1547\n",
      "Epoch 9/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760ms/step - accuracy: 0.6045 - loss: 1.2139\n",
      "Epoch 9: val_accuracy improved from 0.33241 to 0.34697, saving model to models/img_classes_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 840ms/step - accuracy: 0.6045 - loss: 1.2139 - val_accuracy: 0.3470 - val_loss: 1.9081\n",
      "Epoch 10/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753ms/step - accuracy: 0.6177 - loss: 1.1904\n",
      "Epoch 10: val_accuracy improved from 0.34697 to 0.35010, saving model to models/img_classes_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 833ms/step - accuracy: 0.6177 - loss: 1.1904 - val_accuracy: 0.3501 - val_loss: 1.8911\n",
      "Epoch 11/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755ms/step - accuracy: 0.6241 - loss: 1.1680\n",
      "Epoch 11: val_accuracy improved from 0.35010 to 0.36558, saving model to models/img_classes_weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 836ms/step - accuracy: 0.6241 - loss: 1.1680 - val_accuracy: 0.3656 - val_loss: 1.9646\n",
      "Epoch 12/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756ms/step - accuracy: 0.6234 - loss: 1.1538\n",
      "Epoch 12: val_accuracy did not improve from 0.36558\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 836ms/step - accuracy: 0.6234 - loss: 1.1538 - val_accuracy: 0.3042 - val_loss: 2.1731\n",
      "Epoch 13/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757ms/step - accuracy: 0.6291 - loss: 1.1507\n",
      "Epoch 13: val_accuracy did not improve from 0.36558\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 834ms/step - accuracy: 0.6291 - loss: 1.1507 - val_accuracy: 0.3453 - val_loss: 2.0328\n",
      "Epoch 14/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751ms/step - accuracy: 0.6396 - loss: 1.1300\n",
      "Epoch 14: val_accuracy did not improve from 0.36558\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 831ms/step - accuracy: 0.6396 - loss: 1.1300 - val_accuracy: 0.3577 - val_loss: 1.9711\n",
      "Epoch 15/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750ms/step - accuracy: 0.6332 - loss: 1.1361\n",
      "Epoch 15: val_accuracy did not improve from 0.36558\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 826ms/step - accuracy: 0.6333 - loss: 1.1361 - val_accuracy: 0.3591 - val_loss: 2.0835\n",
      "Epoch 16/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753ms/step - accuracy: 0.6496 - loss: 1.1139\n",
      "Epoch 16: val_accuracy did not improve from 0.36558\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 829ms/step - accuracy: 0.6496 - loss: 1.1139 - val_accuracy: 0.3446 - val_loss: 2.2180\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n"
     ]
    }
   ],
   "source": [
    "# 7) Train\n",
    "history = model_2.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7813f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 8 — Two-Head Fusion: IMG_CLASSES CNN + Metadata Head ─────────\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential, layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Flatten, Dropout, Dense, BatchNormalization, Concatenate\n",
    ")\n",
    "# 1) Re-build the CNN “backbone” up through the penultimate Dense(128):\n",
    "cnn_backbone = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "], name=\"img_feature_extractor\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15acce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load the weights you saved from model_2\n",
    "cnn_backbone.load_weights(\"models/img_classes_weights.h5\", by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db9b4e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The layer img_feature_extractor has never been called and thus has no defined output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3) Re-attach the original IMG_CLASSES head so it remains part of the graph\u001b[39;00m\n\u001b[1;32m      2\u001b[0m old_logits \u001b[38;5;241m=\u001b[39m Dense(\n\u001b[1;32m      3\u001b[0m     num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m )(\u001b[43mcnn_backbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/work1/lib/python3.10/site-packages/keras/src/ops/operation.py:288\u001b[0m, in \u001b[0;36mOperation.output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moutput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the output tensor(s) of a layer.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m        Output tensor or list of output tensors.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/work1/lib/python3.10/site-packages/keras/src/ops/operation.py:307\u001b[0m, in \u001b[0;36mOperation._get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m     )\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes) \u001b[38;5;241m>\u001b[39m node_index:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the operation has only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The layer img_feature_extractor has never been called and thus has no defined output."
     ]
    }
   ],
   "source": [
    "# 3) Re-attach the original IMG_CLASSES head so it remains part of the graph\n",
    "old_logits = Dense(\n",
    "    num_classes, activation='softmax', name=\"img_output\"\n",
    ")(cnn_backbone.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Build the metadata branch exactly as in your original image+meta model\n",
    "meta_in = Input(shape=(len(feat_cols),), name=\"meta_input\")\n",
    "m = BatchNormalization()(meta_in)\n",
    "m = Dense(64, activation='relu')(m)\n",
    "m = Dropout(0.3)(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Fuse image features + metadata and append a new “diagnosis” head\n",
    "fusion = Concatenate()([cnn_backbone.output, m])\n",
    "x      = Dense(128, activation='relu')(fusion)\n",
    "x      = Dropout(0.4)(x)\n",
    "new_logits = Dense(\n",
    "    len(classes), activation='softmax', name=\"meta_output\"\n",
    ")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83371b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Assemble the two-head model\n",
    "multihead = Model(\n",
    "    inputs  = [cnn_backbone.input, meta_in],\n",
    "    outputs = [old_logits, new_logits],\n",
    "    name    = \"img_plus_meta_multihead\"\n",
    ")\n",
    "\n",
    "# 7) Freeze everything except the new metadata head\n",
    "for layer in multihead.layers:\n",
    "    if layer.name != \"meta_output\":\n",
    "        layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Compile with two losses (ignore the old head’s loss via weight=0 if desired)\n",
    "multihead.compile(\n",
    "    optimizer    = tf.keras.optimizers.Adam(1e-3),\n",
    "    loss         = [\"categorical_crossentropy\", \"sparse_categorical_crossentropy\"],\n",
    "    loss_weights = [0.0, 1.0],     # train only the metadata head\n",
    "    metrics      = [\"accuracy\", \"accuracy\"]\n",
    ")\n",
    "multihead.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dc79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Wrap your existing `train_ds`/`val_ds` into the format this model expects:\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_multi_ds(ds):\n",
    "    return ds.map(\n",
    "        lambda x, y: (\n",
    "            (x[\"image\"], x[\"meta\"]),     # inputs\n",
    "            (tf.one_hot(y, depth=num_classes), y)  # outputs: [img_onehot, meta_label]\n",
    "        ),\n",
    "        num_parallel_calls=AUTOTUNE\n",
    "    ).prefetch(AUTOTUNE)\n",
    "\n",
    "train_multi_ds = make_multi_ds(train_ds)\n",
    "val_multi_ds   = make_multi_ds(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67badeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Train the metadata head\n",
    "history_multi = multihead.fit(\n",
    "    train_multi_ds,\n",
    "    validation_data=val_multi_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820c139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
