{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3d6de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "62ed1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 ── Imports & Paths\n",
    "import json, pandas as pd, numpy as np, tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Where your allergy images live:\n",
    "IMG_DIRS    = [\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_1\"),\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_2\"),\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_3\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc4d2cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 classes  |  1825 train  /  457 val samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 ── Build & Clean DataFrame\n",
    "meta_df = pd.read_csv(\"/mnt/ssd1/saumia/data/text/metadata.csv\")\n",
    "\n",
    "rows = []\n",
    "for d in IMG_DIRS:\n",
    "    for p in d.glob(\"*.png\"):\n",
    "        parts = p.stem.split(\"_\")\n",
    "        if len(parts) < 4: \n",
    "            continue\n",
    "        # e.g. PAT_1516_1765_530.png → lesion_id = 1765\n",
    "        try:\n",
    "            lesion_id = int(parts[2])\n",
    "        except:\n",
    "            continue\n",
    "        m = meta_df[meta_df[\"lesion_id\"] == lesion_id]\n",
    "        if m.empty:\n",
    "            continue\n",
    "        m = m.iloc[0]\n",
    "        # base row\n",
    "        row = {\n",
    "            \"path\": str(p),\n",
    "            \"label\": m[\"diagnostic\"],\n",
    "            \"age\": m[\"age\"],\n",
    "            \"diameter_1\": m[\"diameter_1\"],\n",
    "            \"diameter_2\": m[\"diameter_2\"],\n",
    "            \"gender_M\": 1.0 if str(m[\"gender\"]).upper()==\"MALE\" else 0.0,\n",
    "            \"region\": m[\"region\"],\n",
    "        }\n",
    "        # the six boolean cols\n",
    "        for col in [\"itch\",\"bleed\",\"elevation\",\"changed\",\"hurt\",\"grew\"]:\n",
    "            row[col] = 1.0 if bool(m.get(col)) else 0.0\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "assert not df.empty, \"No images matched metadata!\"\n",
    "\n",
    "# 1) Drop the all-1 nuisance features\n",
    "constant_cols = [\"itch\",\"bleed\",\"elevation\",\"changed\",\"hurt\",\"grew\"]\n",
    "df = df.drop(columns=constant_cols, errors=\"ignore\")\n",
    "\n",
    "# 2) Normalize the continuous cols to [0,1]\n",
    "for col in [\"age\",\"diameter_1\",\"diameter_2\"]:\n",
    "    if col in df:\n",
    "        mn, mx = df[col].min(), df[col].max()\n",
    "        df[col] = (df[col] - mn) / (mx - mn + 1e-8)\n",
    "\n",
    "# 3) One‐hot the region\n",
    "df = pd.get_dummies(df, columns=[\"region\"], dtype=\"float32\")\n",
    "\n",
    "# 4) Label‐map & feature‐cols\n",
    "classes    = sorted(df[\"label\"].unique())\n",
    "label_map  = {c:i for i,c in enumerate(classes)}\n",
    "feat_cols  = [c for c in df.columns if c not in {\"path\",\"label\"}]\n",
    "\n",
    "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "# persist maps\n",
    "with open(\"label_map_image.json\",\"w\") as f: json.dump(label_map, f, indent=2)\n",
    "with open(\"feature_cols_image.json\",\"w\") as f: json.dump(feat_cols, f, indent=2)\n",
    "\n",
    "# 5) Train/Val split & cast\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "train_df[feat_cols] = train_df[feat_cols].astype(\"float32\")\n",
    "val_df[feat_cols]   = val_df[feat_cols].astype(\"float32\")\n",
    "\n",
    "print(f\"{len(classes)} classes  |  {len(train_df)} train  /  {len(val_df)} val samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ada60213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>diameter_1</th>\n",
       "      <th>diameter_2</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>region_ABDOMEN</th>\n",
       "      <th>region_ARM</th>\n",
       "      <th>region_BACK</th>\n",
       "      <th>region_CHEST</th>\n",
       "      <th>region_EAR</th>\n",
       "      <th>region_FACE</th>\n",
       "      <th>region_FOOT</th>\n",
       "      <th>region_FOREARM</th>\n",
       "      <th>region_HAND</th>\n",
       "      <th>region_LIP</th>\n",
       "      <th>region_NECK</th>\n",
       "      <th>region_NOSE</th>\n",
       "      <th>region_SCALP</th>\n",
       "      <th>region_THIGH</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "      <td>1182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.651986</td>\n",
       "      <td>0.116382</td>\n",
       "      <td>0.122570</td>\n",
       "      <td>0.489848</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>0.083756</td>\n",
       "      <td>0.107445</td>\n",
       "      <td>0.137056</td>\n",
       "      <td>0.038071</td>\n",
       "      <td>0.233503</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.129442</td>\n",
       "      <td>0.050761</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.059222</td>\n",
       "      <td>0.090525</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.031303</td>\n",
       "      <td>1.463621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.162880</td>\n",
       "      <td>0.080234</td>\n",
       "      <td>0.074348</td>\n",
       "      <td>0.500109</td>\n",
       "      <td>0.091628</td>\n",
       "      <td>0.277139</td>\n",
       "      <td>0.309809</td>\n",
       "      <td>0.344052</td>\n",
       "      <td>0.191449</td>\n",
       "      <td>0.423238</td>\n",
       "      <td>0.100288</td>\n",
       "      <td>0.335830</td>\n",
       "      <td>0.219603</td>\n",
       "      <td>0.129028</td>\n",
       "      <td>0.236139</td>\n",
       "      <td>0.287053</td>\n",
       "      <td>0.058099</td>\n",
       "      <td>0.174209</td>\n",
       "      <td>1.374725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age   diameter_1   diameter_2     gender_M  region_ABDOMEN  \\\n",
       "count  1182.000000  1182.000000  1182.000000  1182.000000     1182.000000   \n",
       "mean      0.651986     0.116382     0.122570     0.489848        0.008460   \n",
       "std       0.162880     0.080234     0.074348     0.500109        0.091628   \n",
       "min       0.079545     0.000000     0.000000     0.000000        0.000000   \n",
       "25%       0.556818     0.070000     0.071429     0.000000        0.000000   \n",
       "50%       0.659091     0.100000     0.100000     0.000000        0.000000   \n",
       "75%       0.772727     0.150000     0.142857     1.000000        0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000        1.000000   \n",
       "\n",
       "        region_ARM  region_BACK  region_CHEST   region_EAR  region_FACE  \\\n",
       "count  1182.000000  1182.000000   1182.000000  1182.000000  1182.000000   \n",
       "mean      0.083756     0.107445      0.137056     0.038071     0.233503   \n",
       "std       0.277139     0.309809      0.344052     0.191449     0.423238   \n",
       "min       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000      1.000000     1.000000     1.000000   \n",
       "\n",
       "       region_FOOT  region_FOREARM  region_HAND   region_LIP  region_NECK  \\\n",
       "count  1182.000000     1182.000000  1182.000000  1182.000000  1182.000000   \n",
       "mean      0.010152        0.129442     0.050761     0.016920     0.059222   \n",
       "std       0.100288        0.335830     0.219603     0.129028     0.236139   \n",
       "min       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000        1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       region_NOSE  region_SCALP  region_THIGH     label_id  \n",
       "count  1182.000000   1182.000000   1182.000000  1182.000000  \n",
       "mean      0.090525      0.003384      0.031303     1.463621  \n",
       "std       0.287053      0.058099      0.174209     1.374725  \n",
       "min       0.000000      0.000000      0.000000     0.000000  \n",
       "25%       0.000000      0.000000      0.000000     1.000000  \n",
       "50%       0.000000      0.000000      0.000000     1.000000  \n",
       "75%       0.000000      0.000000      0.000000     1.000000  \n",
       "max       1.000000      1.000000      1.000000     5.000000  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "462bccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dropped rows with missing features: now 1182 train / 299 val\n",
      "Image batch shape: (16, 224, 224, 3)\n",
      "Meta batch shape: (16, 18)\n",
      "Labels: [5 0 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 ── tf.data Pipelines\n",
    "IMG_SIZE   = (224,224)\n",
    "BATCH_SIZE = 16\n",
    "AUTOTUNE   = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(img)\n",
    "\n",
    "def make_dataset(df, shuffle=True):\n",
    "    paths  = df[\"path\"].values\n",
    "    metas  = df[feat_cols].values\n",
    "    labels = df[\"label_id\"].values\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, metas, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(df), seed=42)\n",
    "    def _load(path, meta, label):\n",
    "        return {\"image\": preprocess_image(path), \"meta\": meta}, label\n",
    "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(train_df, shuffle=True)\n",
    "val_ds   = make_dataset(val_df,   shuffle=False)\n",
    "# Clean and coerce feature types\n",
    "for col in feat_cols:\n",
    "    train_df[col] = pd.to_numeric(train_df[col], errors=\"coerce\")\n",
    "    val_df[col]   = pd.to_numeric(val_df[col], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing features\n",
    "train_df = train_df.dropna(subset=feat_cols + [\"label_id\"]).reset_index(drop=True)\n",
    "val_df   = val_df.dropna(subset=feat_cols + [\"label_id\"]).reset_index(drop=True)\n",
    "print(f\"✅ Dropped rows with missing features: now {len(train_df)} train / {len(val_df)} val\")\n",
    "\n",
    "# Ensure all features are float32\n",
    "train_df[feat_cols] = train_df[feat_cols].astype(\"float32\")\n",
    "val_df[feat_cols]   = val_df[feat_cols].astype(\"float32\")\n",
    "\n",
    "# Optional: Clip to [0, 1] for safety\n",
    "train_df[feat_cols] = train_df[feat_cols].clip(0.0, 1.0)\n",
    "val_df[feat_cols]   = val_df[feat_cols].clip(0.0, 1.0)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "for (batch_x, batch_y) in train_ds.take(1):\n",
    "    print(\"Image batch shape:\", batch_x[\"image\"].shape)\n",
    "    print(\"Meta batch shape:\",  batch_x[\"meta\"].shape)\n",
    "    print(\"Labels:\", batch_y.numpy()[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1423a37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"image_meta_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"image_meta_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ meta (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │ meta[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnetb0      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_8       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1344</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ efficientnetb0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">172,160</span> │ concatenate_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ meta (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │         \u001b[38;5;34m72\u001b[0m │ meta[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,216\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnetb0      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │  \u001b[38;5;34m4,049,571\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_8       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1344\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ efficientnetb0[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m172,160\u001b[0m │ concatenate_8[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │        \u001b[38;5;34m774\u001b[0m │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,223,793</span> (16.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,223,793\u001b[0m (16.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">174,186</span> (680.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m174,186\u001b[0m (680.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,607</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,607\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4 ── Build & Compile the Model (Improved Version)\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load EfficientNetB0 with pretrained ImageNet weights\n",
    "base_img = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False, pooling=\"avg\", weights=\"imagenet\")\n",
    "base_img.trainable = False  # Freeze to prevent overfitting on small data\n",
    "\n",
    "# Inputs\n",
    "img_in  = layers.Input(shape=IMG_SIZE + (3,), name=\"image\")\n",
    "meta_in = layers.Input(shape=(len(feat_cols),), name=\"meta\")\n",
    "\n",
    "# Image path\n",
    "x1 = base_img(img_in)\n",
    "\n",
    "# Metadata path\n",
    "x2 = layers.BatchNormalization()(meta_in)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x2)\n",
    "x2 = layers.Dropout(0.3)(x2)\n",
    "\n",
    "# Combine\n",
    "x = layers.concatenate([x1, x2])\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "out = layers.Dense(len(classes), activation=\"softmax\")(x)\n",
    "\n",
    "# Build model\n",
    "model = Model([img_in, meta_in], out, name=\"image_meta_model\")\n",
    "\n",
    "# Compile with gradient clipping and learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR, clipnorm=1.0),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "50cd4dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Any NaNs in final train data? False\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Any NaNs in final train data?\", train_df[feat_cols].isnull().any().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0b69415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.7376 - loss: 0.7547\n",
      "Epoch 1: val_accuracy improved from -inf to 0.63545, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 261ms/step - accuracy: 0.7376 - loss: 0.7546 - val_accuracy: 0.6355 - val_loss: 1.0922 - learning_rate: 6.2500e-06\n",
      "Epoch 2/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.7369 - loss: 0.7521\n",
      "Epoch 2: val_accuracy did not improve from 0.63545\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 252ms/step - accuracy: 0.7369 - loss: 0.7519 - val_accuracy: 0.6321 - val_loss: 1.0926 - learning_rate: 6.2500e-06\n",
      "Epoch 3/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.7364 - loss: 0.7090\n",
      "Epoch 3: val_accuracy did not improve from 0.63545\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 253ms/step - accuracy: 0.7363 - loss: 0.7094 - val_accuracy: 0.6221 - val_loss: 1.0905 - learning_rate: 6.2500e-06\n",
      "Epoch 4/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.7539 - loss: 0.6966\n",
      "Epoch 4: val_accuracy did not improve from 0.63545\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 248ms/step - accuracy: 0.7538 - loss: 0.6971 - val_accuracy: 0.6254 - val_loss: 1.0907 - learning_rate: 6.2500e-06\n",
      "Epoch 5/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.7175 - loss: 0.7529\n",
      "Epoch 5: val_accuracy did not improve from 0.63545\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 249ms/step - accuracy: 0.7177 - loss: 0.7529 - val_accuracy: 0.6221 - val_loss: 1.0891 - learning_rate: 6.2500e-06\n",
      "Epoch 6/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.7078 - loss: 0.7706\n",
      "Epoch 6: val_accuracy did not improve from 0.63545\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 249ms/step - accuracy: 0.7080 - loss: 0.7702 - val_accuracy: 0.6154 - val_loss: 1.0891 - learning_rate: 6.2500e-06\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ── Train (Improved Version)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"image_meta_best.keras\",   # Save only the best\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=33,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3180388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "293f8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/ssd1/saumia/data/images/IMG_CLASSES\"\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "num_classes = len(class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e9470719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21726 images belonging to 10 classes.\n",
      "Found 5427 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    ")\n",
    "\n",
    "train_gen_2 = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_gen_2 = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab3b7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Compute class weights (handle imbalance) ---\n",
    "labels = train_gen_2.classes\n",
    "class_weights_2 = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights_dict_2 = {i: weight for i, weight in enumerate(class_weights_2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "78f36a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Model Definition (Model 2) ---\n",
    "model_2 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7c9e31a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- 5. Training ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m history_2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights_dict_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/work1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/work1/lib/python3.10/site-packages/keras/src/utils/image_utils.py:227\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an image into PIL format.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    A PIL Image instance.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pil_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import PIL.Image. The use of `load_img` requires PIL.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, io\u001b[38;5;241m.\u001b[39mBytesIO):\n\u001b[1;32m    231\u001b[0m     img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(path)\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "# --- 5. Training ---\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history_2 = model_2.fit(\n",
    "    train_gen_2,\n",
    "    validation_data=val_gen_2,\n",
    "    epochs=35,\n",
    "    class_weight=class_weights_dict_2,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5ab10950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /home/saumia/miniconda3/envs/work1/lib/python3.10/site-packages (11.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b8e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
