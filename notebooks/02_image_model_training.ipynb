{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d6de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ed1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 ── Imports & Paths\n",
    "import json, pandas as pd, numpy as np, tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Where your allergy images live:\n",
    "IMG_DIRS    = [\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_1\"),\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_2\"),\n",
    "    Path(\"/mnt/ssd1/saumia/data/images/imgs_part_3\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc4d2cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 classes  |  1184 train  /  297 val samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 ── Build & Clean DataFrame (final, with NaN‐drop before split)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Read your metadata CSV\n",
    "meta_df = pd.read_csv(\"/mnt/ssd1/saumia/data/text/metadata.csv\")\n",
    "\n",
    "# 2) Gather rows by matching image filenames to metadata\n",
    "rows = []\n",
    "for img_dir in IMG_DIRS:\n",
    "    for p in img_dir.glob(\"*.png\"):\n",
    "        parts = p.stem.split(\"_\")\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        try:\n",
    "            lesion_id = int(parts[2])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        m = meta_df[meta_df[\"lesion_id\"] == lesion_id]\n",
    "        if m.empty:\n",
    "            continue\n",
    "        m = m.iloc[0]\n",
    "        row = {\n",
    "            \"path\": str(p),\n",
    "            \"label\": m[\"diagnostic\"],\n",
    "            \"age\": m[\"age\"],\n",
    "            \"diameter_1\": m[\"diameter_1\"],\n",
    "            \"diameter_2\": m[\"diameter_2\"],\n",
    "            \"gender_M\": 1.0 if str(m[\"gender\"]).upper() == \"MALE\" else 0.0,\n",
    "            \"region\": m[\"region\"],\n",
    "        }\n",
    "        # six boolean cols\n",
    "        for col in [\"itch\", \"bleed\", \"elevation\", \"changed\", \"hurt\", \"grew\"]:\n",
    "            row[col] = 1.0 if bool(m.get(col)) else 0.0\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "assert not df.empty, \"No images matched metadata!\"\n",
    "\n",
    "# 3) Drop nuisance features\n",
    "constant_cols = [\"itch\", \"bleed\", \"elevation\", \"changed\", \"hurt\", \"grew\"]\n",
    "df = df.drop(columns=constant_cols, errors=\"ignore\")\n",
    "\n",
    "# 4) Normalize continuous features to [0,1]\n",
    "for col in [\"age\", \"diameter_1\", \"diameter_2\"]:\n",
    "    if col in df:\n",
    "        mn, mx = df[col].min(), df[col].max()\n",
    "        df[col] = (df[col] - mn) / (mx - mn + 1e-8)\n",
    "\n",
    "# 5) One‐hot encode the region column\n",
    "df = pd.get_dummies(df, columns=[\"region\"], dtype=\"float32\")\n",
    "\n",
    "# 6) Create label map and feature‐column list\n",
    "classes   = sorted(df[\"label\"].unique())\n",
    "label_map = {c: i for i, c in enumerate(classes)}\n",
    "feat_cols = [c for c in df.columns if c not in {\"path\", \"label\"}]\n",
    "\n",
    "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "# 7) Persist your mappings\n",
    "with open(\"label_map_image.json\", \"w\")    as f: json.dump(label_map, f, indent=2)\n",
    "with open(\"feature_cols_image.json\", \"w\") as f: json.dump(feat_cols, f, indent=2)\n",
    "\n",
    "# 8) Drop any rows with missing feature or label values\n",
    "df = df.dropna(subset=feat_cols + [\"label_id\"]).reset_index(drop=True)\n",
    "\n",
    "# 9) Stratified train/validation split\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 10) Ensure correct dtypes\n",
    "train_df[feat_cols] = train_df[feat_cols].astype(\"float32\")\n",
    "val_df[feat_cols]   = val_df[feat_cols].astype(\"float32\")\n",
    "train_df[\"label_id\"] = train_df[\"label_id\"].astype(\"int32\")\n",
    "val_df[\"label_id\"]   = val_df[\"label_id\"].astype(\"int32\")\n",
    "\n",
    "print(f\"{len(classes)} classes  |  {len(train_df)} train  /  {len(val_df)} val samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ada60213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>diameter_1</th>\n",
       "      <th>diameter_2</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>region_ABDOMEN</th>\n",
       "      <th>region_ARM</th>\n",
       "      <th>region_BACK</th>\n",
       "      <th>region_CHEST</th>\n",
       "      <th>region_EAR</th>\n",
       "      <th>region_FACE</th>\n",
       "      <th>region_FOOT</th>\n",
       "      <th>region_FOREARM</th>\n",
       "      <th>region_HAND</th>\n",
       "      <th>region_LIP</th>\n",
       "      <th>region_NECK</th>\n",
       "      <th>region_NOSE</th>\n",
       "      <th>region_SCALP</th>\n",
       "      <th>region_THIGH</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.00000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.650031</td>\n",
       "      <td>0.117426</td>\n",
       "      <td>0.124278</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.009291</td>\n",
       "      <td>0.086993</td>\n",
       "      <td>0.11402</td>\n",
       "      <td>0.128378</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>0.231419</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.130068</td>\n",
       "      <td>0.048142</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>0.059966</td>\n",
       "      <td>0.096284</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>1.461993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.166620</td>\n",
       "      <td>0.086309</td>\n",
       "      <td>0.082412</td>\n",
       "      <td>0.500166</td>\n",
       "      <td>0.095979</td>\n",
       "      <td>0.281944</td>\n",
       "      <td>0.31797</td>\n",
       "      <td>0.334652</td>\n",
       "      <td>0.180749</td>\n",
       "      <td>0.421918</td>\n",
       "      <td>0.100204</td>\n",
       "      <td>0.336520</td>\n",
       "      <td>0.214156</td>\n",
       "      <td>0.119012</td>\n",
       "      <td>0.237525</td>\n",
       "      <td>0.295105</td>\n",
       "      <td>0.058050</td>\n",
       "      <td>0.180749</td>\n",
       "      <td>1.382260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age   diameter_1   diameter_2     gender_M  region_ABDOMEN  \\\n",
       "count  1184.000000  1184.000000  1184.000000  1184.000000     1184.000000   \n",
       "mean      0.650031     0.117426     0.124278     0.493243        0.009291   \n",
       "std       0.166620     0.086309     0.082412     0.500166        0.095979   \n",
       "min       0.079545     0.000000     0.000000     0.000000        0.000000   \n",
       "25%       0.545455     0.070000     0.071429     0.000000        0.000000   \n",
       "50%       0.659091     0.100000     0.100000     0.000000        0.000000   \n",
       "75%       0.772727     0.150000     0.142857     1.000000        0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000        1.000000   \n",
       "\n",
       "        region_ARM  region_BACK  region_CHEST   region_EAR  region_FACE  \\\n",
       "count  1184.000000   1184.00000   1184.000000  1184.000000  1184.000000   \n",
       "mean      0.086993      0.11402      0.128378     0.033784     0.231419   \n",
       "std       0.281944      0.31797      0.334652     0.180749     0.421918   \n",
       "min       0.000000      0.00000      0.000000     0.000000     0.000000   \n",
       "25%       0.000000      0.00000      0.000000     0.000000     0.000000   \n",
       "50%       0.000000      0.00000      0.000000     0.000000     0.000000   \n",
       "75%       0.000000      0.00000      0.000000     0.000000     0.000000   \n",
       "max       1.000000      1.00000      1.000000     1.000000     1.000000   \n",
       "\n",
       "       region_FOOT  region_FOREARM  region_HAND   region_LIP  region_NECK  \\\n",
       "count  1184.000000     1184.000000  1184.000000  1184.000000  1184.000000   \n",
       "mean      0.010135        0.130068     0.048142     0.014358     0.059966   \n",
       "std       0.100204        0.336520     0.214156     0.119012     0.237525   \n",
       "min       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000        0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000        1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       region_NOSE  region_SCALP  region_THIGH     label_id  \n",
       "count  1184.000000   1184.000000   1184.000000  1184.000000  \n",
       "mean      0.096284      0.003378      0.033784     1.461993  \n",
       "std       0.295105      0.058050      0.180749     1.382260  \n",
       "min       0.000000      0.000000      0.000000     0.000000  \n",
       "25%       0.000000      0.000000      0.000000     1.000000  \n",
       "50%       0.000000      0.000000      0.000000     1.000000  \n",
       "75%       0.000000      0.000000      0.000000     1.000000  \n",
       "max       1.000000      1.000000      1.000000     5.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "462bccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dropped rows with missing features: now 1184 train / 297 val\n",
      "Image batch shape: (16, 224, 224, 3)\n",
      "Meta batch shape: (16, 18)\n",
      "Labels: [3 2 1 3 1 1 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 05:52:43.510408: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 ── tf.data Pipelines\n",
    "IMG_SIZE   = (224,224)\n",
    "BATCH_SIZE = 16\n",
    "AUTOTUNE   = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(img)\n",
    "\n",
    "def make_dataset(df, shuffle=True):\n",
    "    paths  = df[\"path\"].values\n",
    "    metas  = df[feat_cols].values\n",
    "    labels = df[\"label_id\"].values\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, metas, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(df), seed=42)\n",
    "    def _load(path, meta, label):\n",
    "        return {\"image\": preprocess_image(path), \"meta\": meta}, label\n",
    "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(train_df, shuffle=True)\n",
    "val_ds   = make_dataset(val_df,   shuffle=False)\n",
    "# Clean and coerce feature types\n",
    "for col in feat_cols:\n",
    "    train_df[col] = pd.to_numeric(train_df[col], errors=\"coerce\")\n",
    "    val_df[col]   = pd.to_numeric(val_df[col], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing features\n",
    "train_df = train_df.dropna(subset=feat_cols + [\"label_id\"]).reset_index(drop=True)\n",
    "val_df   = val_df.dropna(subset=feat_cols + [\"label_id\"]).reset_index(drop=True)\n",
    "print(f\"✅ Dropped rows with missing features: now {len(train_df)} train / {len(val_df)} val\")\n",
    "\n",
    "# Ensure all features are float32\n",
    "train_df[feat_cols] = train_df[feat_cols].astype(\"float32\")\n",
    "val_df[feat_cols]   = val_df[feat_cols].astype(\"float32\")\n",
    "\n",
    "# Optional: Clip to [0, 1] for safety\n",
    "train_df[feat_cols] = train_df[feat_cols].clip(0.0, 1.0)\n",
    "val_df[feat_cols]   = val_df[feat_cols].clip(0.0, 1.0)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "for (batch_x, batch_y) in train_ds.take(1):\n",
    "    print(\"Image batch shape:\", batch_x[\"image\"].shape)\n",
    "    print(\"Meta batch shape:\",  batch_x[\"meta\"].shape)\n",
    "    print(\"Labels:\", batch_y.numpy()[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1423a37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"image_meta_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"image_meta_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ meta (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │ meta[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnetb0      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1344</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ efficientnetb0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">172,160</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ meta (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │         \u001b[38;5;34m72\u001b[0m │ meta[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,216\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnetb0      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │  \u001b[38;5;34m4,049,571\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1344\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ efficientnetb0[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m172,160\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │        \u001b[38;5;34m774\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,223,793</span> (16.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,223,793\u001b[0m (16.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">174,186</span> (680.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m174,186\u001b[0m (680.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,607</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,607\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4 ── Build & Compile the Model (Improved Version)\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load EfficientNetB0 with pretrained ImageNet weights\n",
    "base_img = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False, pooling=\"avg\", weights=\"imagenet\")\n",
    "base_img.trainable = False  # Freeze to prevent overfitting on small data\n",
    "\n",
    "# Inputs\n",
    "img_in  = layers.Input(shape=IMG_SIZE + (3,), name=\"image\")\n",
    "meta_in = layers.Input(shape=(len(feat_cols),), name=\"meta\")\n",
    "\n",
    "# Image path\n",
    "x1 = base_img(img_in)\n",
    "\n",
    "# Metadata path\n",
    "x2 = layers.BatchNormalization()(meta_in)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x2)\n",
    "x2 = layers.Dropout(0.3)(x2)\n",
    "\n",
    "# Combine\n",
    "x = layers.concatenate([x1, x2])\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "out = layers.Dense(len(classes), activation=\"softmax\")(x)\n",
    "\n",
    "# Build model\n",
    "model = Model([img_in, meta_in], out, name=\"image_meta_model\")\n",
    "\n",
    "\n",
    "\n",
    "# Compile with gradient clipping and learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50cd4dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Any NaNs in final train data? False\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Any NaNs in final train data?\", train_df[feat_cols].isnull().any().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b69415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.2888 - loss: 1.7759\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55892, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 302ms/step - accuracy: 0.2909 - loss: 1.7726 - val_accuracy: 0.5589 - val_loss: 1.2571 - learning_rate: 1.0000e-04\n",
      "Epoch 2/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.5467 - loss: 1.2526\n",
      "Epoch 2: val_accuracy improved from 0.55892 to 0.56566, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 286ms/step - accuracy: 0.5468 - loss: 1.2528 - val_accuracy: 0.5657 - val_loss: 1.2153 - learning_rate: 1.0000e-04\n",
      "Epoch 3/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.5739 - loss: 1.2095\n",
      "Epoch 3: val_accuracy improved from 0.56566 to 0.57239, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 276ms/step - accuracy: 0.5739 - loss: 1.2093 - val_accuracy: 0.5724 - val_loss: 1.1854 - learning_rate: 1.0000e-04\n",
      "Epoch 4/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.5860 - loss: 1.1934\n",
      "Epoch 4: val_accuracy did not improve from 0.57239\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 276ms/step - accuracy: 0.5859 - loss: 1.1929 - val_accuracy: 0.5690 - val_loss: 1.1525 - learning_rate: 1.0000e-04\n",
      "Epoch 5/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.6021 - loss: 1.1178\n",
      "Epoch 5: val_accuracy improved from 0.57239 to 0.58586, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 283ms/step - accuracy: 0.6020 - loss: 1.1177 - val_accuracy: 0.5859 - val_loss: 1.1317 - learning_rate: 1.0000e-04\n",
      "Epoch 6/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.6178 - loss: 1.0764\n",
      "Epoch 6: val_accuracy improved from 0.58586 to 0.60606, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 282ms/step - accuracy: 0.6177 - loss: 1.0765 - val_accuracy: 0.6061 - val_loss: 1.1074 - learning_rate: 1.0000e-04\n",
      "Epoch 7/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.6048 - loss: 1.0642\n",
      "Epoch 7: val_accuracy improved from 0.60606 to 0.60943, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 281ms/step - accuracy: 0.6048 - loss: 1.0643 - val_accuracy: 0.6094 - val_loss: 1.0898 - learning_rate: 1.0000e-04\n",
      "Epoch 8/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.6554 - loss: 0.9736\n",
      "Epoch 8: val_accuracy did not improve from 0.60943\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 268ms/step - accuracy: 0.6550 - loss: 0.9742 - val_accuracy: 0.6027 - val_loss: 1.0729 - learning_rate: 1.0000e-04\n",
      "Epoch 9/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.6605 - loss: 1.0139\n",
      "Epoch 9: val_accuracy did not improve from 0.60943\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 273ms/step - accuracy: 0.6604 - loss: 1.0137 - val_accuracy: 0.6094 - val_loss: 1.0646 - learning_rate: 1.0000e-04\n",
      "Epoch 10/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.6332 - loss: 0.9909\n",
      "Epoch 10: val_accuracy did not improve from 0.60943\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 288ms/step - accuracy: 0.6333 - loss: 0.9909 - val_accuracy: 0.6094 - val_loss: 1.0558 - learning_rate: 1.0000e-04\n",
      "Epoch 11/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6536 - loss: 0.9439\n",
      "Epoch 11: val_accuracy improved from 0.60943 to 0.61616, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 293ms/step - accuracy: 0.6537 - loss: 0.9441 - val_accuracy: 0.6162 - val_loss: 1.0341 - learning_rate: 1.0000e-04\n",
      "Epoch 12/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6894 - loss: 0.8994\n",
      "Epoch 12: val_accuracy did not improve from 0.61616\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 284ms/step - accuracy: 0.6891 - loss: 0.8998 - val_accuracy: 0.6162 - val_loss: 1.0330 - learning_rate: 1.0000e-04\n",
      "Epoch 13/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.6849 - loss: 0.8983\n",
      "Epoch 13: val_accuracy did not improve from 0.61616\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 282ms/step - accuracy: 0.6850 - loss: 0.8983 - val_accuracy: 0.6162 - val_loss: 1.0178 - learning_rate: 1.0000e-04\n",
      "Epoch 14/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.6766 - loss: 0.8689\n",
      "Epoch 14: val_accuracy improved from 0.61616 to 0.62626, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 289ms/step - accuracy: 0.6767 - loss: 0.8692 - val_accuracy: 0.6263 - val_loss: 1.0181 - learning_rate: 1.0000e-04\n",
      "Epoch 15/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.6651 - loss: 0.8962\n",
      "Epoch 15: val_accuracy improved from 0.62626 to 0.63636, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 291ms/step - accuracy: 0.6655 - loss: 0.8957 - val_accuracy: 0.6364 - val_loss: 1.0049 - learning_rate: 1.0000e-04\n",
      "Epoch 16/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.7124 - loss: 0.7881\n",
      "Epoch 16: val_accuracy improved from 0.63636 to 0.63973, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 288ms/step - accuracy: 0.7122 - loss: 0.7887 - val_accuracy: 0.6397 - val_loss: 0.9990 - learning_rate: 1.0000e-04\n",
      "Epoch 17/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7215 - loss: 0.7934\n",
      "Epoch 17: val_accuracy did not improve from 0.63973\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 284ms/step - accuracy: 0.7214 - loss: 0.7938 - val_accuracy: 0.6330 - val_loss: 0.9933 - learning_rate: 1.0000e-04\n",
      "Epoch 18/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.7142 - loss: 0.8174\n",
      "Epoch 18: val_accuracy did not improve from 0.63973\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 279ms/step - accuracy: 0.7142 - loss: 0.8174 - val_accuracy: 0.6330 - val_loss: 0.9941 - learning_rate: 1.0000e-04\n",
      "Epoch 19/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7326 - loss: 0.7625\n",
      "Epoch 19: val_accuracy did not improve from 0.63973\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 276ms/step - accuracy: 0.7322 - loss: 0.7632 - val_accuracy: 0.6397 - val_loss: 0.9713 - learning_rate: 1.0000e-04\n",
      "Epoch 20/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.7167 - loss: 0.7959\n",
      "Epoch 20: val_accuracy improved from 0.63973 to 0.65320, saving model to image_meta_best.keras\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 286ms/step - accuracy: 0.7167 - loss: 0.7960 - val_accuracy: 0.6532 - val_loss: 0.9691 - learning_rate: 1.0000e-04\n",
      "Epoch 21/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7387 - loss: 0.7392\n",
      "Epoch 21: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 278ms/step - accuracy: 0.7384 - loss: 0.7399 - val_accuracy: 0.6465 - val_loss: 0.9703 - learning_rate: 1.0000e-04\n",
      "Epoch 22/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7310 - loss: 0.7563\n",
      "Epoch 22: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 280ms/step - accuracy: 0.7309 - loss: 0.7563 - val_accuracy: 0.6532 - val_loss: 0.9683 - learning_rate: 1.0000e-04\n",
      "Epoch 23/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7542 - loss: 0.7118\n",
      "Epoch 23: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 279ms/step - accuracy: 0.7540 - loss: 0.7123 - val_accuracy: 0.6431 - val_loss: 0.9674 - learning_rate: 1.0000e-04\n",
      "Epoch 24/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.7309 - loss: 0.7327\n",
      "Epoch 24: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 273ms/step - accuracy: 0.7311 - loss: 0.7327 - val_accuracy: 0.6498 - val_loss: 0.9614 - learning_rate: 1.0000e-04\n",
      "Epoch 25/33\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7543 - loss: 0.7010\n",
      "Epoch 25: val_accuracy did not improve from 0.65320\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 278ms/step - accuracy: 0.7543 - loss: 0.7010 - val_accuracy: 0.6431 - val_loss: 0.9659 - learning_rate: 1.0000e-04\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ── Train (Improved Version)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"image_meta_best.keras\",   # Save only the best\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=33,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3180388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293f8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/ssd1/saumia/data/images/IMG_CLASSES\"\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "num_classes = len(class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9470719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21726 images belonging to 10 classes.\n",
      "Found 5427 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    ")\n",
    "\n",
    "train_gen_2 = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_gen_2 = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3b7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Compute class weights (handle imbalance) ---\n",
    "labels = train_gen_2.classes\n",
    "class_weights_2 = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights_dict_2 = {i: weight for i, weight in enumerate(class_weights_2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f36a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saumia/miniconda3/envs/work1/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-05-22 23:24:39.537695: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Model Definition (Model 2) ---\n",
    "model_2 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e31a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792ms/step - accuracy: 0.4302 - loss: 1.7993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saumia/miniconda3/envs/work1/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 870ms/step - accuracy: 0.4302 - loss: 1.7992 - val_accuracy: 0.2349 - val_loss: 2.0937\n",
      "Epoch 2/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 870ms/step - accuracy: 0.4891 - loss: 1.5734 - val_accuracy: 0.2782 - val_loss: 2.0078\n",
      "Epoch 3/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 868ms/step - accuracy: 0.5194 - loss: 1.4854 - val_accuracy: 0.2729 - val_loss: 2.0458\n",
      "Epoch 4/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m595s\u001b[0m 876ms/step - accuracy: 0.5307 - loss: 1.4303 - val_accuracy: 0.2851 - val_loss: 2.0911\n",
      "Epoch 5/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m593s\u001b[0m 872ms/step - accuracy: 0.5398 - loss: 1.4055 - val_accuracy: 0.3077 - val_loss: 1.9759\n",
      "Epoch 6/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 874ms/step - accuracy: 0.5589 - loss: 1.3456 - val_accuracy: 0.3121 - val_loss: 1.9722\n",
      "Epoch 7/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 871ms/step - accuracy: 0.5780 - loss: 1.3073 - val_accuracy: 0.3118 - val_loss: 2.0330\n",
      "Epoch 8/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 867ms/step - accuracy: 0.5938 - loss: 1.2614 - val_accuracy: 0.3479 - val_loss: 1.8609\n",
      "Epoch 9/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 870ms/step - accuracy: 0.5904 - loss: 1.2621 - val_accuracy: 0.3451 - val_loss: 1.9920\n",
      "Epoch 10/35\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790ms/step - accuracy: 0.6024 - loss: 1.2294"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# --- 5. Training ---\n",
    "# Ensure the model is running on the GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history_2 = model_2.fit(\n",
    "        train_gen_2,\n",
    "        validation_data=val_gen_2,\n",
    "        epochs=35,\n",
    "        class_weight=class_weights_dict_2,\n",
    "        callbacks=[early_stop]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image model\n",
    "model.save(\"./image_model.keras\")\n",
    "model_2.save(\"./image_model_2.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
